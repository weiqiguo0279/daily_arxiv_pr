# è¶‹åŠ¿åˆ†ææ¨¡å—ä½¿ç”¨æŒ‡å—

## ğŸ“– æ¨¡å—ä»‹ç»

`trend_analyzer.py` æ˜¯è´Ÿè´£åˆ†æè®ºæ–‡é›†åˆï¼Œç”Ÿæˆç ”ç©¶è¶‹åŠ¿æŠ¥å‘Šçš„æ ¸å¿ƒæ¨¡å—ã€‚å®ƒç»“åˆäº†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ†ææŠ€æœ¯å’Œ LLM çš„æ·±åº¦ç†è§£èƒ½åŠ›ï¼Œæä¾›å…¨é¢çš„ç ”ç©¶è¶‹åŠ¿æ´å¯Ÿã€‚

## ğŸ¯ ä¸»è¦åŠŸèƒ½

### 1. å…³é”®è¯æå– (Keyword Extraction)

ä½¿ç”¨ **TF-IDF** ç®—æ³•æå–æœ€é‡è¦çš„ç ”ç©¶å…³é”®è¯ï¼š

- è¯†åˆ«é«˜é¢‘æŠ€æœ¯æœ¯è¯­
- æ”¯æŒå•è¯å’ŒäºŒå…ƒç»„ (bigrams)
- è‡ªåŠ¨è¿‡æ»¤åœç”¨è¯
- è¿”å›å¸¦æƒé‡çš„å…³é”®è¯åˆ—è¡¨

**è¾“å‡ºç¤ºä¾‹**:
```json
{
  "keyword": "transformer",
  "score": 0.2845
}
```

### 2. ä¸»é¢˜æå– (Topic Modeling)

ä½¿ç”¨ **LDA (Latent Dirichlet Allocation)** å‘ç°æ½œåœ¨ç ”ç©¶ä¸»é¢˜ï¼š

- è‡ªåŠ¨èšç±»ç›¸ä¼¼è®ºæ–‡
- è¯†åˆ«ç ”ç©¶å­æ–¹å‘
- æå–æ¯ä¸ªä¸»é¢˜çš„ä»£è¡¨æ€§å…³é”®è¯

**è¾“å‡ºç¤ºä¾‹**:
```json
{
  "topic_id": 1,
  "keywords": ["neural", "network", "training", "optimization"],
  "weights": [0.15, 0.12, 0.10, 0.08]
}
```

### 3. è¯äº‘ç”Ÿæˆ (Word Cloud)

ç”Ÿæˆç¾è§‚çš„è¯äº‘å›¾ï¼Œç›´è§‚å±•ç¤ºç ”ç©¶çƒ­ç‚¹ï¼š

- é«˜åˆ†è¾¨ç‡å›¾ç‰‡ (1600x800)
- è‡ªå®šä¹‰é…è‰²æ–¹æ¡ˆ
- è‡ªåŠ¨è°ƒæ•´å­—ä½“å¤§å°
- ä¿å­˜ä¸º PNG æ ¼å¼

**ç”Ÿæˆè·¯å¾„**: `data/analysis/wordcloud_YYYY-MM-DD.png`

### 4. ç»Ÿè®¡åˆ†æ (Statistical Analysis)

ç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Šï¼š

- è®ºæ–‡ã€ä½œè€…ã€ç±»åˆ«æ•°é‡ç»Ÿè®¡
- ç±»åˆ«åˆ†å¸ƒ
- é«˜äº§ä½œè€…è¯†åˆ«
- é«˜é¢‘è¯ç»Ÿè®¡
- æ—¶é—´åˆ†å¸ƒåˆ†æ

### 5. LLM æ·±åº¦åˆ†æ (LLM Deep Analysis)

ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æï¼Œç”Ÿæˆï¼š

#### a) å½“å‰ç ”ç©¶çƒ­ç‚¹ (Current Hotspots)
- è¯†åˆ«æœ€çƒ­é—¨çš„ç ”ç©¶æ–¹å‘
- åˆ†æçƒ­åº¦åŸå› 
- å…³æ³¨åº¦è¶‹åŠ¿

#### b) æŠ€æœ¯è¶‹åŠ¿ä¸æ¼”è¿› (Technical Trends)
- ä¸»æµæŠ€æœ¯æ–¹æ³•å’Œæ¶æ„
- æ­£åœ¨å…´èµ·çš„æ–°æŠ€æœ¯
- æŠ€æœ¯æ¼”è¿›è·¯å¾„åˆ†æ

#### c) æœªæ¥å‘å±•æ–¹å‘ (Future Directions)
- é¢„æµ‹æœªæ¥ 6-12 ä¸ªæœˆçš„ç ”ç©¶æ–¹å‘
- è¯†åˆ«æœªå……åˆ†æ¢ç´¢çš„æœºä¼š
- å¯èƒ½çš„æŠ€æœ¯çªç ´ç‚¹

#### d) åˆ›æ–°ç ”ç©¶æƒ³æ³• (Research Ideas)
- 5-8 ä¸ªå…·ä½“çš„ç ”ç©¶æƒ³æ³•
- æ¯ä¸ªæƒ³æ³•åŒ…æ‹¬ï¼š
  - æ ¸å¿ƒåˆ›æ–°ç‚¹
  - ä»·å€¼åˆ†æ
  - æŠ€æœ¯å®ç°è·¯å¾„
  - åº”ç”¨åœºæ™¯

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºç¡€ç”¨æ³•

```python
from src.utils import load_config, load_env, setup_logging, load_json
from src.analyzer.trend_analyzer import TrendAnalyzer
from src.summarizer.llm_factory import LLMClientFactory

# åˆå§‹åŒ–
load_env()
config = load_config()
logger = setup_logging(config)

# åŠ è½½è®ºæ–‡æ•°æ®
papers_data = load_json('data/papers/latest.json')
papers = papers_data['papers']

# åŠ è½½è®ºæ–‡æ€»ç»“ï¼ˆå¯é€‰ï¼‰
summaries_data = load_json('data/summaries/latest.json')
summaries = summaries_data.get('summaries', [])

# åˆ›å»º LLM å®¢æˆ·ç«¯
llm_client = LLMClientFactory.create_client(config)

# åˆ›å»ºåˆ†æå™¨
analyzer = TrendAnalyzer(config, llm_client)

# æ‰§è¡Œå®Œæ•´åˆ†æ
analysis = analyzer.analyze(papers, summaries)

# æ‰“å°æ‘˜è¦
analyzer.print_analysis_summary(analysis)
```

### é«˜çº§ç”¨æ³•

#### 1. åªæå–å…³é”®è¯ï¼ˆä¸éœ€è¦ LLMï¼‰

```python
analyzer = TrendAnalyzer(config, llm_client=None)
keywords = analyzer._extract_keywords(papers, top_n=50)

for kw in keywords[:10]:
    print(f"{kw['keyword']}: {kw['score']:.4f}")
```

#### 2. åªç”Ÿæˆè¯äº‘

```python
analyzer = TrendAnalyzer(config, llm_client=None)
wordcloud_path = analyzer._generate_wordcloud(papers)
print(f"è¯äº‘å·²ä¿å­˜: {wordcloud_path}")
```

#### 3. è‡ªå®šä¹‰ä¸»é¢˜æ•°é‡

```python
topics = analyzer._extract_topics(papers, n_topics=10)
```

#### 4. è·å–ç»Ÿè®¡ä¿¡æ¯

```python
statistics = analyzer._generate_statistics(papers, summaries)
print(f"æ€»è®ºæ–‡æ•°: {statistics['total_papers']}")
print(f"ç±»åˆ«åˆ†å¸ƒ: {statistics['category_distribution']}")
```

## ğŸ“Š è¾“å‡ºæ–‡ä»¶

### 1. JSON æ ¼å¼ (`data/analysis/analysis_YYYY-MM-DD.json`)

å®Œæ•´çš„åˆ†æç»“æœï¼ŒåŒ…å«æ‰€æœ‰æ•°æ®ï¼š

```json
{
  "date": "2025-10-13",
  "paper_count": 20,
  "keywords": [...],
  "topics": [...],
  "statistics": {...},
  "wordcloud_path": "...",
  "llm_analysis": {...},
  "generated_at": "2025-10-13T14:30:00"
}
```

### 2. Markdown æŠ¥å‘Š (`data/analysis/report_YYYY-MM-DD.md`)

æ ¼å¼åŒ–çš„åˆ†ææŠ¥å‘Šï¼Œé€‚åˆé˜…è¯»å’Œåˆ†äº«ï¼š

```markdown
# ç ”ç©¶è¶‹åŠ¿åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¥æœŸ**: 2025-10-13
**åˆ†æè®ºæ–‡æ•°**: 20

## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ
...

## ğŸ”¥ ç ”ç©¶çƒ­ç‚¹åˆ†æ
...

## ğŸ“ˆ æŠ€æœ¯è¶‹åŠ¿ä¸æ¼”è¿›
...

## ğŸ”® æœªæ¥å‘å±•æ–¹å‘
...

## ğŸ’¡ åˆ›æ–°ç ”ç©¶æƒ³æ³•
...
```

### 3. è¯äº‘å›¾ç‰‡ (`data/analysis/wordcloud_YYYY-MM-DD.png`)

é«˜åˆ†è¾¨ç‡çš„è¯äº‘å¯è§†åŒ–å›¾ç‰‡

### 4. æœ€æ–°åˆ†æ (`data/analysis/latest.json`)

ä¾› Web æœåŠ¡ä½¿ç”¨çš„æœ€æ–°åˆ†æç»“æœ

## âš™ï¸ é…ç½®è¯´æ˜

åˆ†æå™¨ä¼šä½¿ç”¨ `config.yaml` ä¸­çš„ LLM é…ç½®ï¼š

```yaml
llm:
  provider: "openai"  # æˆ– gemini, claude, deepseek, vllm
  
  openai:
    api_key: "your-key"
    model: "gpt-4"
    temperature: 0.7
    max_tokens: 3000  # åˆ†æéœ€è¦æ›´å¤š tokens
```

### æ¨èé…ç½®

å¯¹äºè¶‹åŠ¿åˆ†æï¼Œæ¨èä½¿ç”¨ï¼š

- **æ¨¡å‹**: GPT-4, Claude 3 Opus, Gemini Pro
- **æ¸©åº¦**: 0.7-0.8 (éœ€è¦ä¸€å®šåˆ›é€ æ€§)
- **Max Tokens**: 2000-3000 (åˆ†ææŠ¥å‘Šè¾ƒé•¿)

## ğŸ” åˆ†æè´¨é‡ä¼˜åŒ–

### 1. è®ºæ–‡æ•°é‡

- **æœ€å°‘**: 10 ç¯‡è®ºæ–‡ï¼ˆåŸºç¡€åˆ†æï¼‰
- **æ¨è**: 20-50 ç¯‡è®ºæ–‡ï¼ˆè¾ƒå…¨é¢ï¼‰
- **æœ€ä½³**: 50+ ç¯‡è®ºæ–‡ï¼ˆæ·±åº¦åˆ†æï¼‰

### 2. åŒ…å«è®ºæ–‡æ€»ç»“

æä¾›è®ºæ–‡æ€»ç»“ä¼šæ˜¾è‘—æå‡åˆ†æè´¨é‡ï¼š

```python
# å…ˆæ€»ç»“è®ºæ–‡
summaries = summarizer.summarize_papers(papers)

# å†åˆ†æè¶‹åŠ¿ï¼ˆåŒ…å«æ€»ç»“ï¼‰
analysis = analyzer.analyze(papers, summaries)
```

### 3. é€‰æ‹©åˆé€‚çš„ LLM

ä¸åŒä»»åŠ¡çš„æ¨èï¼š

| ä»»åŠ¡ | æ¨èæ¨¡å‹ | åŸå›  |
|------|---------|------|
| ç ”ç©¶çƒ­ç‚¹ | GPT-4, Claude 3 | æ·±åº¦ç†è§£èƒ½åŠ›å¼º |
| è¶‹åŠ¿é¢„æµ‹ | GPT-4, Gemini Pro | æ¨ç†èƒ½åŠ›å¥½ |
| åˆ›æ–°æƒ³æ³• | Claude 3, GPT-4 | åˆ›é€ æ€§å¼º |
| å¿«é€Ÿåˆ†æ | GPT-3.5, Gemini | æˆæœ¬ä½ï¼Œé€Ÿåº¦å¿« |

## ğŸ“ å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯ 1: æ¯æ—¥ç ”ç©¶è·Ÿè¸ª

```python
# æ¯å¤©è¿è¡Œ
papers = fetcher.fetch_papers(days_back=1)
summaries = summarizer.summarize_papers(papers)
analysis = analyzer.analyze(papers, summaries)
```

### åœºæ™¯ 2: å‘¨æŠ¥ç”Ÿæˆ

```python
# æ¯å‘¨è¿è¡Œ
papers = fetcher.fetch_papers(days_back=7)
summaries = summarizer.summarize_papers(papers)
analysis = analyzer.analyze(papers, summaries)

# ä½¿ç”¨ç”Ÿæˆçš„ Markdown æŠ¥å‘Šä½œä¸ºå‘¨æŠ¥
```

### åœºæ™¯ 3: é€‰é¢˜ç ”ç©¶

```python
# è·å–å¤§é‡è®ºæ–‡
papers = fetcher.fetch_papers(days_back=30)
config['arxiv']['max_results'] = 100

# æ·±åº¦åˆ†æ
analysis = analyzer.analyze(papers, summaries)

# ä» research_ideas ä¸­é€‰æ‹©è¯¾é¢˜
ideas = analysis['llm_analysis']['research_ideas']
```

### åœºæ™¯ 4: ç«å“åˆ†æ

```python
# é…ç½®ç‰¹å®šå…³é”®è¯
config['arxiv']['keywords'] = ['competitor_method']

# åˆ†æç«äº‰å¯¹æ‰‹çš„ç ”ç©¶æ–¹å‘
papers = fetcher.fetch_papers()
analysis = analyzer.analyze(papers)
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: è¯äº‘ç”Ÿæˆå¤±è´¥

**å¯èƒ½åŸå› **:
- ç¼ºå°‘å­—ä½“æ–‡ä»¶
- matplotlib åç«¯é—®é¢˜

**è§£å†³æ–¹æ³•**:
```python
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
```

### Q2: LLM åˆ†æè¶…æ—¶

**è§£å†³æ–¹æ³•**:
- å‡å°‘è®ºæ–‡æ•°é‡ï¼ˆåªç”¨å‰ 30 ç¯‡ï¼‰
- å¢åŠ  `max_tokens` é™åˆ¶
- ä¼˜åŒ–æç¤ºè¯é•¿åº¦

### Q3: å…³é”®è¯ä¸å‡†ç¡®

**ä¼˜åŒ–æ–¹æ³•**:
```python
# æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯
analyzer.stop_words.update(['custom', 'stopword'])

# è°ƒæ•´ TF-IDF å‚æ•°
keywords = analyzer._extract_keywords(
    papers, 
    top_n=100,  # å¢åŠ æ•°é‡
    min_df=3    # æé«˜æœ€å°æ–‡æ¡£é¢‘ç‡
)
```

### Q4: ä¸»é¢˜é‡å ä¸¥é‡

**è§£å†³æ–¹æ³•**:
```python
# è°ƒæ•´ä¸»é¢˜æ•°é‡
topics = analyzer._extract_topics(papers, n_topics=8)

# æˆ–å¢åŠ è®ºæ–‡æ•°é‡ä»¥æé«˜åŒºåˆ†åº¦
```

### Q5: åˆ†ææŠ¥å‘Šè´¨é‡ä¸é«˜

**æ”¹è¿›å»ºè®®**:

1. **æä¾›æ›´å¤šä¸Šä¸‹æ–‡**:
```python
# åŒ…å«è®ºæ–‡æ€»ç»“
summaries = summarizer.summarize_papers(papers)
analysis = analyzer.analyze(papers, summaries)
```

2. **ä½¿ç”¨æ›´å¥½çš„æ¨¡å‹**:
```yaml
llm:
  provider: "openai"
  openai:
    model: "gpt-4"  # è€Œä¸æ˜¯ gpt-3.5-turbo
```

3. **è°ƒæ•´æç¤ºè¯**:
ä¿®æ”¹ `_build_analysis_prompt()` æ–¹æ³•ä»¥é€‚åº”ä½ çš„éœ€æ±‚

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. å®Œæ•´å·¥ä½œæµ

```python
# 1. çˆ¬å–è®ºæ–‡
papers = fetcher.fetch_papers(days_back=7)

# 2. æ€»ç»“è®ºæ–‡
summaries = summarizer.summarize_papers(papers)

# 3. è¶‹åŠ¿åˆ†æ
analysis = analyzer.analyze(papers, summaries)

# 4. ç”ŸæˆæŠ¥å‘Š
analyzer.print_analysis_summary(analysis)
```

### 2. å®šæœŸæ›´æ–°è¯äº‘

```bash
# æ¯å‘¨ä¸€æ¬¡æ›´æ–°è¯äº‘èƒŒæ™¯å›¾
python -c "
from src.analyzer.trend_analyzer import TrendAnalyzer
analyzer = TrendAnalyzer(config)
analyzer._generate_wordcloud(papers)
"
```

### 3. è‡ªå®šä¹‰åˆ†ææ·±åº¦

```python
# æµ…å±‚åˆ†æï¼ˆå¿«é€Ÿï¼‰
analysis_light = {
    'keywords': analyzer._extract_keywords(papers),
    'wordcloud': analyzer._generate_wordcloud(papers),
    'statistics': analyzer._generate_statistics(papers)
}

# æ·±åº¦åˆ†æï¼ˆå®Œæ•´ï¼‰
analysis_deep = analyzer.analyze(papers, summaries)
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [è®ºæ–‡çˆ¬å–æŒ‡å—](arxiv_fetcher_guide.md)
- [è®ºæ–‡æ€»ç»“æŒ‡å—](paper_summarizer_guide.md)
- [é…ç½®æŒ‡å—](config_guide.md)

## ğŸ”— æŠ€æœ¯å‚è€ƒ

- [TF-IDF ç®—æ³•](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
- [LDA ä¸»é¢˜æ¨¡å‹](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)
- [WordCloud æ–‡æ¡£](https://amueller.github.io/word_cloud/)
- [scikit-learn æ–‡æ¡£](https://scikit-learn.org/)
