#!/usr/bin/env python3
"""
æµ‹è¯•è¶‹åŠ¿åˆ†æåŠŸèƒ½

è¿™ä¸ªè„šæœ¬ç”¨äºæµ‹è¯•è¶‹åŠ¿åˆ†ææ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils import load_config, load_env, setup_logging, load_json
from src.analyzer.trend_analyzer import TrendAnalyzer
from src.summarizer.llm_factory import LLMClientFactory


def test_keyword_extraction():
    """æµ‹è¯•å…³é”®è¯æå–"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: å…³é”®è¯æå–")
    print("=" * 60)
    
    load_env()
    config = load_config()
    logger = setup_logging(config)
    
    # åŠ è½½è®ºæ–‡æ•°æ®
    papers_data = load_json('data/papers/latest.json')
    if not papers_data:
        print("âŒ æœªæ‰¾åˆ°è®ºæ–‡æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ: python main.py")
        return False
    
    papers = papers_data.get('papers', [])
    print(f"åŠ è½½äº† {len(papers)} ç¯‡è®ºæ–‡")
    
    # åˆ›å»ºåˆ†æå™¨ï¼ˆä¸éœ€è¦ LLMï¼‰
    analyzer = TrendAnalyzer(config, llm_client=None)
    
    # æå–å…³é”®è¯
    keywords = analyzer._extract_keywords(papers, top_n=20)
    
    print("\nâœ… å…³é”®è¯æå–æˆåŠŸ")
    print(f"æå–äº† {len(keywords)} ä¸ªå…³é”®è¯")
    print("\nTop 20 å…³é”®è¯:")
    for i, kw in enumerate(keywords, 1):
        print(f"  {i}. {kw['keyword']} (æƒé‡: {kw['score']:.4f})")
    
    return True


def test_topic_extraction():
    """æµ‹è¯•ä¸»é¢˜æå–"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: ä¸»é¢˜æå–")
    print("=" * 60)
    
    load_env()
    config = load_config()
    
    papers_data = load_json('data/papers/latest.json')
    if not papers_data:
        return False
    
    papers = papers_data.get('papers', [])
    
    analyzer = TrendAnalyzer(config, llm_client=None)
    
    # æå–ä¸»é¢˜
    topics = analyzer._extract_topics(papers, n_topics=5)
    
    print("\nâœ… ä¸»é¢˜æå–æˆåŠŸ")
    print(f"æå–äº† {len(topics)} ä¸ªä¸»é¢˜")
    print("\nä¸»é¢˜è¯¦æƒ…:")
    for topic in topics:
        print(f"\nä¸»é¢˜ {topic['topic_id']}:")
        print(f"  å…³é”®è¯: {', '.join(topic['keywords'][:8])}")
    
    return True


def test_wordcloud_generation():
    """æµ‹è¯•è¯äº‘ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: è¯äº‘ç”Ÿæˆ")
    print("=" * 60)
    
    load_env()
    config = load_config()
    
    papers_data = load_json('data/papers/latest.json')
    if not papers_data:
        return False
    
    papers = papers_data.get('papers', [])
    
    analyzer = TrendAnalyzer(config, llm_client=None)
    
    # ç”Ÿæˆè¯äº‘
    wordcloud_path = analyzer._generate_wordcloud(papers)
    
    print(f"\nâœ… è¯äº‘ç”ŸæˆæˆåŠŸ")
    print(f"ä¿å­˜ä½ç½®: {wordcloud_path}")
    
    import os
    if os.path.exists(wordcloud_path):
        size = os.path.getsize(wordcloud_path)
        print(f"æ–‡ä»¶å¤§å°: {size / 1024:.2f} KB")
        return True
    else:
        print("âŒ è¯äº‘æ–‡ä»¶æœªæ‰¾åˆ°")
        return False


def test_statistics():
    """æµ‹è¯•ç»Ÿè®¡åˆ†æ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: ç»Ÿè®¡åˆ†æ")
    print("=" * 60)
    
    load_env()
    config = load_config()
    
    papers_data = load_json('data/papers/latest.json')
    if not papers_data:
        return False
    
    papers = papers_data.get('papers', [])
    
    # å°è¯•åŠ è½½æ€»ç»“
    summaries_data = load_json('data/summaries/latest.json')
    summaries = summaries_data.get('summaries', []) if summaries_data else None
    
    analyzer = TrendAnalyzer(config, llm_client=None)
    
    # ç”Ÿæˆç»Ÿè®¡
    statistics = analyzer._generate_statistics(papers, summaries)
    
    print("\nâœ… ç»Ÿè®¡åˆ†ææˆåŠŸ")
    print(f"\nç»Ÿè®¡ç»“æœ:")
    print(f"  æ€»è®ºæ–‡æ•°: {statistics['total_papers']}")
    print(f"  æ€»ä½œè€…æ•°: {statistics['total_authors']}")
    print(f"  ç ”ç©¶ç±»åˆ«: {statistics['total_categories']}")
    
    print(f"\nç±»åˆ«åˆ†å¸ƒ (Top 5):")
    for i, (cat, count) in enumerate(list(statistics['category_distribution'].items())[:5], 1):
        print(f"  {i}. {cat}: {count} ç¯‡")
    
    print(f"\né«˜äº§ä½œè€…:")
    for author, count in list(statistics['prolific_authors'].items())[:5]:
        print(f"  - {author}: {count} ç¯‡")
    
    return True


def test_llm_analysis():
    """æµ‹è¯• LLM æ·±åº¦åˆ†æ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: LLM æ·±åº¦åˆ†æ")
    print("=" * 60)
    
    load_env()
    config = load_config()
    logger = setup_logging(config)
    
    papers_data = load_json('data/papers/latest.json')
    if not papers_data:
        return False
    
    papers = papers_data.get('papers', [])[:10]  # åªç”¨å‰10ç¯‡æµ‹è¯•
    print(f"ä½¿ç”¨ {len(papers)} ç¯‡è®ºæ–‡è¿›è¡Œæµ‹è¯•")
    
    # å°è¯•åŠ è½½æ€»ç»“
    summaries_data = load_json('data/summaries/latest.json')
    summaries = summaries_data.get('summaries', [])[:10] if summaries_data else []
    
    try:
        # åˆ›å»º LLM å®¢æˆ·ç«¯
        llm_client = LLMClientFactory.create_client(config)
        
        analyzer = TrendAnalyzer(config, llm_client)
        
        # æå–å…³é”®è¯å’Œä¸»é¢˜
        keywords = analyzer._extract_keywords(papers, top_n=20)
        topics = analyzer._extract_topics(papers, n_topics=3)
        
        # ç”Ÿæˆ LLM åˆ†æ
        print("\næ­£åœ¨è°ƒç”¨ LLM ç”Ÿæˆåˆ†æ...")
        llm_analysis = analyzer._generate_llm_analysis(papers, summaries, keywords, topics)
        
        print("\nâœ… LLM åˆ†ææˆåŠŸ")
        print("\nåˆ†æç»“æœé¢„è§ˆ:")
        for key in ['hotspots', 'trends', 'future_directions', 'research_ideas']:
            content = llm_analysis.get(key, '')
            if content:
                preview = content[:150] + "..." if len(content) > 150 else content
                print(f"\n{key}: {preview}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ LLM åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_full_analysis():
    """æµ‹è¯•å®Œæ•´åˆ†ææµç¨‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 6: å®Œæ•´åˆ†ææµç¨‹")
    print("=" * 60)
    
    load_env()
    config = load_config()
    logger = setup_logging(config)
    
    papers_data = load_json('data/papers/latest.json')
    if not papers_data:
        return False
    
    papers = papers_data.get('papers', [])
    
    # åŠ è½½æ€»ç»“
    summaries_data = load_json('data/summaries/latest.json')
    summaries = summaries_data.get('summaries', []) if summaries_data else []
    
    try:
        # åˆ›å»º LLM å®¢æˆ·ç«¯
        llm_client = LLMClientFactory.create_client(config)
        
        analyzer = TrendAnalyzer(config, llm_client)
        
        # æ‰§è¡Œå®Œæ•´åˆ†æ
        print(f"\nåˆ†æ {len(papers)} ç¯‡è®ºæ–‡...")
        analysis = analyzer.analyze(papers, summaries)
        
        if analysis:
            print("\nâœ… å®Œæ•´åˆ†ææˆåŠŸ")
            analyzer.print_analysis_summary(analysis)
            
            print("\nç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"  - JSON: data/analysis/analysis_{analysis['date']}.json")
            print(f"  - Markdown: data/analysis/report_{analysis['date']}.md")
            print(f"  - è¯äº‘: {analysis['wordcloud_path']}")
            
            return True
        else:
            print("âŒ åˆ†æç»“æœä¸ºç©º")
            return False
            
    except Exception as e:
        print(f"\nâŒ å®Œæ•´åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("ğŸ§ª è¶‹åŠ¿åˆ†ææ¨¡å—æµ‹è¯•")
    print("=" * 70)
    
    results = {
        'å…³é”®è¯æå–': False,
        'ä¸»é¢˜æå–': False,
        'è¯äº‘ç”Ÿæˆ': False,
        'ç»Ÿè®¡åˆ†æ': False,
        'LLM æ·±åº¦åˆ†æ': False,
        'å®Œæ•´åˆ†ææµç¨‹': False
    }
    
    try:
        # åŸºç¡€æµ‹è¯•ï¼ˆä¸éœ€è¦ LLMï¼‰
        results['å…³é”®è¯æå–'] = test_keyword_extraction()
        results['ä¸»é¢˜æå–'] = test_topic_extraction()
        results['è¯äº‘ç”Ÿæˆ'] = test_wordcloud_generation()
        results['ç»Ÿè®¡åˆ†æ'] = test_statistics()
        
        # LLM ç›¸å…³æµ‹è¯•
        print("\n" + "=" * 60)
        print("æ˜¯å¦æµ‹è¯• LLM åŠŸèƒ½ï¼Ÿ(éœ€è¦é…ç½® API Key)")
        print("=" * 60)
        
        user_input = input("\nç»§ç»­ LLM æµ‹è¯•? (y/n, é»˜è®¤ n): ").strip().lower()
        
        if user_input == 'y':
            results['LLM æ·±åº¦åˆ†æ'] = test_llm_analysis()
            results['å®Œæ•´åˆ†ææµç¨‹'] = test_full_analysis()
        else:
            print("\nè·³è¿‡ LLM æµ‹è¯•")
        
        # æ€»ç»“
        print("\n" + "=" * 70)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
        print("=" * 70)
        
        for test_name, passed in results.items():
            status = "âœ… é€šè¿‡" if passed else "â­ï¸  è·³è¿‡" if not passed and test_name in ['LLM æ·±åº¦åˆ†æ', 'å®Œæ•´åˆ†ææµç¨‹'] else "âŒ å¤±è´¥"
            print(f"  {test_name}: {status}")
        
        passed_count = sum(1 for v in results.values() if v)
        total_count = len(results)
        
        print("\n" + "=" * 70)
        print(f"æµ‹è¯•é€šè¿‡ç‡: {passed_count}/{total_count}")
        print("=" * 70)
        
        print("\nğŸ’¡ æç¤º:")
        print("  - æŸ¥çœ‹ç”Ÿæˆçš„è¯äº‘: data/analysis/wordcloud_*.png")
        print("  - æŸ¥çœ‹åˆ†ææŠ¥å‘Š: data/analysis/report_*.md")
        print("  - è¿è¡Œå®Œæ•´æµç¨‹: python main.py")
        print()
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
