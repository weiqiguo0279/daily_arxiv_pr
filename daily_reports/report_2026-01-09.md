# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì

**Êó•Êúü**: 2026-01-09
**ËÆ∫ÊñáÊï∞Èáè**: 10 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


**ÊÄªÁªì**:
- [Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles](https://arxiv.org/abs/2511.17675)
  - Navneet Singh, Shiva Raj Pokhrel
  - Publish Date: 2025.11.21
  - Task: Prediction
  - Datasets: [Waymo Open Motion Dataset](https://waymo.com/open/)
  - SummaryÔºö
    - Proposes a compact hybrid quantum architecture for trajectory forecasting, using an ego-centric, lane-aligned frame to predict residual corrections to a kinematic baseline.
    - The model combines a quantum attention encoder, a quantum feedforward stack, and a Fourier-based decoder to generate 16 trajectory hypotheses in a single pass, with mode confidences from the latent spectrum.
    - Trained with Simultaneous Perturbation Stochastic Approximation (SPSA), it achieves improved minADE and minFDE on the Waymo Open Motion Dataset, demonstrating stable optimization from small, shallow quantum circuits.

---


**ÊÄªÁªì**:
- [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
  - Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo
  - Publish Date: 2025.10.06
  - Task: Planning
  - Datasets: [nuPlan](https://www.nuscenes.org/nuplan)
  - SummaryÔºö
    - Introduces DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT) automatically generated from nuPlan, designed to investigate the causal relationship between reasoning and planning in VLM driving agents.
    - Presents evidence for a consistent causal disconnect (Reasoning-Planning Decoupling Hypothesis), where planning primarily relies on priors rather than the generated reasoning, and proposes a novel, training-free diagnostic probe to measure an agent's reliance on priors.

---


**ÊÄªÁªì**:
- [Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities](https://arxiv.org/abs/2509.15400)
  - Eric Aislan Antonelo, Gustavo Claudio Karl Couto, Christian M√∂ller
  - Publish Date: 2025.09.18
  - Task: Planning
  - Datasets: [CARLA](https://carla.org/)
  - SummaryÔºö
    - Explores Implicit Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to capture multimodal driving decisions where multiple valid actions exist for the same scenario.
    - Proposes Data-Augmented IBC (DA-IBC), which improves learning by perturbing expert actions for counterexamples and using better initialization for derivative-free inference.
    - Demonstrates that DA-IBC outperforms standard IBC in CARLA simulator urban driving tasks, with learned energy landscapes representing multimodal action distributions that standard Behavior Cloning fails to achieve.

---


**ÊÄªÁªì**:
- [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
  - Tim Kreuzer, Jelena Zdravkovic, Panagiotis Papapetrou
  - Publisher: Stockholm University
  - Publish Date: 2025.08.26
  - Task: Prediction
  - SummaryÔºö
    - PAX-TS, a model-agnostic post-hoc algorithm for explaining time series forecasting models via localized input perturbations, producing multi-granular explanations.
    - The method characterizes cross-channel correlations for multivariate forecasts and identifies distinct explanation patterns that correlate with model performance.

---


**ÊÄªÁªì**:
- [PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography](https://arxiv.org/abs/2508.04062)
  - Yichi Zhang, Wenbo Zhang, Zehui Ling, Gang Feng, Sisi Peng, Deshu Chen, Yuchen Liu, Hongwei Zhang, Shuqi Wang, Lanlan Li, Limei Han, Yuan Cheng, Zixin Hu, Yuan Qi, Le Xue
  - Publish Date: 2025.08.06
  - Task: VQA
  - Datasets: [PET2Rep](https://arxiv.org/abs/2508.04062)
  - SummaryÔºö
    - Introduces PET2Rep, a large-scale benchmark for evaluating general and medical Vision-Language Models (VLMs) on radiology report generation for PET images, addressing a gap in existing benchmarks focused on structural imaging.
    - Proposes novel clinical efficacy metrics to evaluate the quality of radiotracer uptake pattern descriptions in generated reports, alongside standard natural language generation metrics.
    - Conducts a comprehensive evaluation of 30 state-of-the-art VLMs, revealing their current insufficiency for practical PET report generation and identifying key areas for future development.

---


**ÊÄªÁªì**:
- [Goal-Based Vision-Language Driving](https://arxiv.org/abs/2507.23042)
  - Santosh Patapati, Trisanth Srinivasan
  - Publisher: Not specified in provided text
  - Publish Date: 2025.07.30
  - Task: Planning
  - Datasets: [nuScenes](https://www.nuscenes.org/), [Waymo](https://waymo.com/open/), [MD-NEX Outdoor](https://github.com/autonomousvision/md-nex)
  - SummaryÔºö
    - Introduces NovaDrive, a single-branch vision-language architecture for autonomous driving that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints using a lightweight, two-stage cross-attention block.
    - Employs a novel smoothness loss to discourage abrupt steering and speed changes, eliminating the need for recurrent memory, and fine-tunes a large vision-language backbone for real-time inference.
    - Demonstrates state-of-the-art performance on benchmarks, raising success rates and path efficiency while reducing collisions, with contributions from waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion.

---


**ÊÄªÁªì**:
- [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
  - Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak
  - Publisher: Carnegie Mellon University
  - Publish Date: 2025.07.21
  - Project Page: [Diffusion Scaling](https://diffusion-scaling.github.io)
  - Code: [Diffusion Scaling](https://diffusion-scaling.github.io)
  - SummaryÔºö
    - Systematically studies masked diffusion language models in data-constrained settings, finding they outperform autoregressive models when compute is abundant but data is scarce.
    - Derives new scaling laws for diffusion models and a closed-form expression for the compute threshold where diffusion begins to outperform autoregressive modeling.
    - Explains the advantage via diffusion's randomized masking objective, which acts as implicit data augmentation over token orderings, a benefit lacking in fixed left-to-right autoregressive factorization.

---


**ÊÄªÁªì**:
- [Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving](https://arxiv.org/abs/2506.23771)
  - Guizhe Jin, Zhuoren Li, Bo Leng, Ran Yu, Lu Xiong, Chen Sun
  - Publisher: Tongji University
  - Publish Date: 2025.06.30
  - Task: Planning, Control
  - Datasets: [HighD](https://www.highd-dataset.com/)
  - SummaryÔºö
    - Proposes a multi-timescale hierarchical reinforcement learning approach for autonomous driving, using a unified-trained hierarchical policy to output long-timescale motion guidance and short-timescale control commands.
    - Introduces hybrid actions to explicitly represent motion guidance, capturing multimodal driving behaviors on structured roads and supporting incremental low-level state updates.
    - Designs a hierarchical safety mechanism to ensure multi-timescale safety, demonstrating improved driving efficiency, action consistency, and safety in highway multi-lane scenarios.

---


**ÊÄªÁªì**:
- [Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency](https://arxiv.org/abs/2506.07497)
  - Xiangyu Guo, Zhanqian Wu, Kaixin Xiong, Ziyang Xu, Lijun Zhou, Gangwei Xu, Shaoqing Xu, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang
  - Publisher: Huazhong University of Science and Technology, Xiaomi EV
  - Publish Date: 2025.06.09
  - Task: Generation
  - Datasets: [nuScenes](https://www.nuscenes.org/)
  - SummaryÔºö
    - Genesis, a unified framework for joint generation of multi-view driving videos and LiDAR sequences with spatio-temporal and cross-modal consistency.
    - Employs a two-stage architecture integrating a DiT-based video diffusion model and a BEV-aware LiDAR generator, coupled through a shared latent space.
    - Introduces DataCrafter, a captioning module for scene-level and instance-level supervision, achieving state-of-the-art performance on nuScenes and benefiting downstream tasks.

---


**ÊÄªÁªì**:
- [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/abs/2505.19381)
  - Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuweng Heng, Hao Jiang, Yunda Dong, Zongzheng Zhang, Xianda Guo, Hao Sun, Hao Zhao
  - Publish Date: 2025.05.26
  - Task: Planning
  - SummaryÔºö
    - Proposes DiffVLA, a novel hybrid sparse-dense diffusion policy for autonomous driving, empowered by a Vision-Language Model (VLM).
    - Explores sparse diffusion representation for efficient multi-modal driving behavior and improves trajectory generation through deep interaction across agent, map instances, and VLM output.
    - Achieves superior performance in the Autonomous Grand Challenge 2025, scoring 45.0 PDMS.

---
