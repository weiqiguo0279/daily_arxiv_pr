# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì

**Êó•Êúü**: 2026-01-09
**ËÆ∫ÊñáÊï∞Èáè**: 5 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


**ÊÄªÁªì**:
- [Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles](https://arxiv.org/abs/2511.17675)
  - Navneet Singh, Shiva Raj Pokhrel
  - Publish Date: 2025.11.21
  - Task: Prediction
  - Datasets: [Waymo Open Motion Dataset](https://waymo.com/open/)
  - SummaryÔºö
    - Proposes a compact hybrid quantum architecture for trajectory forecasting, operating in an ego-centric, lane-aligned frame to predict residual corrections to a kinematic baseline.
    - The model combines a quantum attention encoder, a quantum feedforward stack, and a Fourier-based decoder to generate multiple trajectory hypotheses in a single pass, with mode confidences derived from the latent spectrum.
    - Trained with Simultaneous Perturbation Stochastic Approximation (SPSA), the model achieves improved minADE and minFDE on the Waymo Open Motion Dataset, demonstrating reliable multi-modal forecasts from small, shallow quantum circuits.

---


**ÊÄªÁªì**:
- [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
  - Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo
  - Publish Date: 2025.10.06
  - Task: Planning
  - Datasets: [nuPlan](https://www.nuscenes.org/nuplan)
  - SummaryÔºö
    - Introduces DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT) automatically generated from nuPlan to investigate the causal link between reasoning and planning in VLM driving agents.
    - Presents evidence of a consistent causal disconnect between reasoning and planning, proposing the Reasoning-Planning Decoupling Hypothesis, where reasoning is an ancillary byproduct rather than a causal mediator for planning.
    - Proposes a novel, training-free diagnostic probe to measure an agent's reliance on priors by evaluating planning robustness against minor input perturbations.

---


**ÊÄªÁªì**:
- [Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities](https://arxiv.org/abs/2509.15400)
  - Eric Aislan Antonelo, Gustavo Claudio Karl Couto, Christian M√∂ller
  - Publish Date: 2025.09.18
  - Task: Navigation
  - Datasets: [CARLA](https://carla.org/)
  - SummaryÔºö
    - Explores Implicit Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to capture multimodal driving decisions where multiple valid actions exist for the same scenario.
    - Proposes Data-Augmented IBC (DA-IBC), which improves learning by perturbing expert actions for counterexamples and using better initialization for inference.
    - Experiments in CARLA with Bird's-Eye View inputs show DA-IBC outperforms standard IBC in urban driving tasks, learning energy landscapes that represent multimodal action distributions.

---


**ÊÄªÁªì**:
- [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
  - Tim Kreuzer, Jelena Zdravkovic, Panagiotis Papapetrou
  - Publisher: Stockholm University
  - Publish Date: 2025.08.26
  - Task: Prediction
  - SummaryÔºö
    - PAX-TS, a model-agnostic post-hoc algorithm to explain time series forecasting models via localized input perturbations, producing multi-granular explanations.
    - The method characterizes cross-channel correlations for multivariate forecasts and can illustrate model mechanisms in different levels of detail to answer practical questions.

---


**ÊÄªÁªì**:
- [PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography](https://arxiv.org/abs/2508.04062)
  - Yichi Zhang, Wenbo Zhang, Zehui Ling, Gang Feng, Sisi Peng, Deshu Chen, Yuchen Liu, Hongwei Zhang, Shuqi Wang, Lanlan Li, Limei Han, Yuan Cheng, Zixin Hu, Yuan Qi, Le Xue
  - Publish Date: 2025.08.06
  - Task: VQA
  - Datasets: [PET2Rep](https://arxiv.org/abs/2508.04062)
  - SummaryÔºö
    - Introduces PET2Rep, a large-scale benchmark for evaluating general and medical Vision-Language Models (VLMs) on radiology report generation for PET images, addressing a gap in existing benchmarks focused on molecular imaging.
    - Proposes a series of clinical efficacy metrics to evaluate the quality of radiotracer uptake pattern descriptions in generated reports, alongside standard natural language generation metrics.
    - Conducts a head-to-head comparison of 30 state-of-the-art VLMs, finding current models perform poorly on the PET report generation task and identifying key insufficiencies for advancing medical applications.

---
