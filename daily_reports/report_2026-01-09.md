# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì

**Êó•Êúü**: 2026-01-09
**ËÆ∫ÊñáÊï∞Èáè**: 10 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


**ÊÄªÁªì**:
- [Joint Encoding of KV-Cache Blocks for Scalable LLM Serving](https://arxiv.org/abs/2601.03067)
  - Joseph Kampeas, Emir Haleva
  - Publisher: (Inferred from abstract/context: Not explicitly stated)
  - Publish Date: 2026.01.06
  - Code: [kv_fast_fusion](https://github.com/sef1/kv_fast_fusion)
  - Task: (Inferred from abstract: Not a standard autonomous driving task; LLM serving optimization)
  - SummaryÔºö
    - Proposes joint encoding of KV-cache blocks, fusing similar blocks across requests and input chunks into shared representations to alleviate the KV-cache memory bottleneck in LLM serving.
    - The method achieves up to 4.38x KV-cache compression with negligible accuracy loss and improves token throughput by ~40% in a real serving benchmark.

---


**ÊÄªÁªì**:
- [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
  - Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo
  - Publish Date: 2025.10.06
  - Task: Reasoning, Planning
  - Datasets: [nuPlan](https://www.nuscenes.org/nuplan)
  - SummaryÔºö
    - Introduces DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT) automatically generated from nuPlan to investigate the causal link between reasoning and planning in VLM driving agents.
    - Presents evidence for a Reasoning-Planning Decoupling Hypothesis, showing planning primarily relies on priors rather than the generated reasoning, and proposes a novel training-free diagnostic probe to measure an agent's reliance on priors.

---


**ÊÄªÁªì**:
- [Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning](https://arxiv.org/abs/2508.06000)
  - Wei Xiang, Ziyue Lei, Haoyuan Che, Fangyuan Ye, Xueting Wu, Lingyun Sun
  - Publish Date: 2025.08.08
  - Task: Control
  - SummaryÔºö
    - Introduces FlightAxis, a tool integrating an LLM with Electrical Muscle Stimulation (EMS) for kinesthetic assistance in flight skill acquisition, using an "Align-Analyze-Adjust" strategy.
    - Demonstrates high user acceptance of LLM-mediated body control, significantly reducing task completion times and enhancing trainee awareness of operation flaws and engagement.

---


**ÊÄªÁªì**:
- [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/abs/2505.19381)
  - Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuweng Heng, Hao Jiang, Yunda Dong, Zongzheng Zhang, Xianda Guo, Hao Sun, Hao Zhao
  - Publish Date: 2025.05.26
  - Task: Planning
  - SummaryÔºö
    - Proposes Diff-VLA, a novel hybrid sparse-dense diffusion policy for autonomous driving, empowered by a Vision-Language Model (VLM).
    - Explores sparse diffusion representation for efficient multi-modal driving behavior and improves trajectory generation through deep interaction across agent, map instances, and VLM output.
    - Achieves superior performance in the Autonomous Grand Challenge 2025, with a score of 45.0 PDMS.

---


**ÊÄªÁªì**:
- [EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs](https://arxiv.org/abs/2505.17139)
  - Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai
  - Publish Date: 2025.05.22
  - Project Page: [EarthSE](https://huggingface.co/ai-earth)
  - Task: Reasoning
  - Datasets: [Earth-Iron](https://huggingface.co/ai-earth), [Earth-Silver](https://huggingface.co/ai-earth), [Earth-Gold](https://huggingface.co/ai-earth)
  - SummaryÔºö
    - EarthSE, a comprehensive and professional benchmark for evaluating the capabilities of Large Language Models (LLMs) in Earth scientific exploration, spanning from fundamental to advanced levels.
    - The benchmark introduces two Question Answering datasets (Earth-Iron and Earth-Silver) and a novel open-ended multi-turn dialogue dataset (Earth-Gold) to assess capabilities including methodology induction, limitation analysis, and concept proposal.
    - Extensive experiments on 11 leading LLMs reveal significant limitations in their scientific exploration capabilities across five Earth spheres, 114 disciplines, and 11 task categories.

---


**ÊÄªÁªì**:
- [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
  - Jennifer D'Souza, Hamed Babaei Giglou, Quentin M√ºnch
  - Publish Date: 2025.05.20
  - Task: Reasoning
  - SummaryÔºö
    - YESciEval, an open-source framework for robust LLM-as-a-Judge evaluation of scientific question-answering, combining fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias.
    - Introduces and releases multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs, enabling scalable and cost-free evaluation independent of proprietary models and human feedback.

---


**ÊÄªÁªì**:
- [Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data](https://arxiv.org/abs/2505.02130)
  - Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan
  - Publish Date: 2025.05.04
  - Code: [LLM4Exploration](https://github.com/millioniron/LLM_exploration)
  - Task: Reasoning
  - SummaryÔºö
    - An empirical study exploring how LLMs process graph-structured data from the perspective of attention mechanisms, uncovering unique phenomena in their attention application.
    - Key findings show LLMs struggle to model inter-node relationships in graphs due to architectural constraints and their attention distribution fails to adapt to graph topology nuances.
    - Proposes that intermediate-state attention windows improve LLM training for graphs and can transition to fully connected windows during inference.

---


**ÊÄªÁªì**:
- [Evolution of AI in Education: Agentic Workflows](https://arxiv.org/abs/2504.20082)
  - Firuz Kamalov, David Santandreu Calonge, Linda Smail, Dilshod Azizov, Dimple R. Thadani, Theresa Kwong, Amara Atif
  - Publish Date: 2025.04.25
  - Task: Reasoning
  - SummaryÔºö
    - Examines agentic workflows in education through four paradigms: reflection, planning, tool use, and multi-agent collaboration, analyzing their advantages, applications, and challenges.
    - Presents a proof-of-concept multi-agent framework for automated essay scoring, with preliminary results suggesting improved consistency over stand-alone LLMs.

---


**ÊÄªÁªì**:
- [Exploring human-SAV interaction using LLMs: The impact of psychological factors on user experience](https://arxiv.org/abs/2504.16548)
  - Lirui Guo, Michael G. Burke, Wynita M. Griggs
  - Publish Date: 2025.04.23
  - Task: Reasoning
  - SummaryÔºö
    - Investigates how LLM-powered conversational SAV agents, with varying anthropomorphism and psychological ownership triggers, affect user perceptions, experience, and adoption intentions.
    - Quantitative and qualitative results show that more anthropomorphic, ownership-inducing agents improve perceptions of human-like qualities and response positivity, highlighting the importance of personalization in SAV interaction design.

---


**ÊÄªÁªì**:
- [Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing](https://arxiv.org/abs/2504.13629)
  - Cong William Lin, Wu Zhu
  - Publish Date: 2025.04.18
  - Task: Reasoning
  - SummaryÔºö
    - Investigates the impact of AI-assisted generative revisions (e.g., ChatGPT) on research manuscripts, analyzing heterogeneous adoption patterns and their influence on writing convergence.
    - Develops a novel classification framework using fine-tuned LLMs to detect ChatGPT-revised texts, revealing substantial disparities in adoption across disciplines, gender, native language, and career stage.
    - Shows that LLM usage enhances clarity, conciseness, and formal writing conventions, driving convergence in academic writing with pronounced stylistic shifts among early adopters, male researchers, non-native speakers, and junior scholars.

---
