# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì

**Êó•Êúü**: 2026-01-09
**ËÆ∫ÊñáÊï∞Èáè**: 10 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


- [A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving](https://arxiv.org/abs/2601.03519)
  - Liangdong Zhang, Yiming Nie, Haoyang Li, Fanjie Kong, Baobao Zhang, Shunxin Huang, Kai Fu, Chen Min, Liang Xiao
  - Publish Date: 2026.01.07
  - Task: Planning
  - Datasets: [RELLIS-3D](https://github.com/unmannedlab/RELLIS-3D)
  - SummaryÔºö
    - Proposes OFF-EMMA, an end-to-end multimodal framework for off-road autonomous driving, addressing insufficient spatial perception and unstable reasoning in VLA models.
    - Introduces a visual prompt block using semantic segmentation masks to enhance spatial understanding and a chain-of-thought with self-consistency (COT-SC) reasoning strategy for robust trajectory planning.

---


- [Dichotomous Diffusion Policy Optimization](https://arxiv.org/abs/2601.00898)
  - Ruiming Liang, Yinan Zheng, Kexin Zheng, Tianyi Tan, Jianxiong Li, Liyuan Mao, Zhihao Wang, Guang Chen, Hangjun Ye, Jingjing Liu, Jinqiao Wang, Xianyuan Zhan
  - Publish Date: 2025.12.31
  - Task: Planning, Control, End-to-End
  - Datasets: [ExORL](https://github.com/rail-berkeley/rlpd?tab=readme-ov-file#exorl-d4rl), [OGBench](https://github.com/OpenGVLab/ogbench), [NAVSIM](https://github.com/autonomousvision/navsim)
  - SummaryÔºö
    - Proposes DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm for stable and controllable diffusion policy optimization.
    - Introduces a greedified policy regularization scheme that decomposes the optimal policy into a pair of dichotomous policies for reward maximization and minimization.
    - Demonstrates effectiveness in offline/online RL benchmarks and trains a large VLA model for end-to-end autonomous driving on the NAVSIM benchmark.

---


- [Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning](https://arxiv.org/abs/2512.24426)
  - Zhenghao "Mark" Peng, Wenhao Ding, Yurong You, Yuxiao Chen, Wenjie Luo, Thomas Tian, Yulong Cao, Apoorva Sharma, Danfei Xu, Boris Ivanovic, Boyi Li, Bolei Zhou, Yan Wang, Marco Pavone
  - Publisher: Stanford University, NVIDIA, MIT, University of California, Los Angeles, University of California, Berkeley, University of Michigan, Ann Arbor
  - Publish Date: 2025.12.30
  - Task: Planning, Reasoning, End-to-End
  - SummaryÔºö
    - Introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution by performing counterfactual reasoning.
    - Proposes a rollout-filter-label pipeline to efficiently mine high-value scenes and label counterfactual reasoning traces for training, enabling the model to learn self-correction.
    - Demonstrates significant improvements in trajectory accuracy (up to 17.6%) and safety metrics (20.5%), with adaptive reasoning that activates primarily in challenging scenarios.

---


- [Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future](https://arxiv.org/abs/2512.16760)
  - Tianshuai Hu, Xiaolu Liu, Song Wang, Yiyao Zhu, Ao Liang, Lingdong Kong, Guoyang Zhao, Zeying Gong, Jun Cen, Zhiyu Huang, Xiaoshuai Hao, Linfeng Li, Hang Song, Xiangtai Li, Jun Ma, Shaojie Shen, Jianke Zhu, Dacheng Tao, Ziwei Liu, Junwei Liang
  - Publish Date: 2025.12.18
  - Task: End-to-End
  - SummaryÔºö
    - Provides a structured characterization of the emerging Vision-Language-Action (VLA) landscape for autonomous driving, tracing evolution from Vision-Action models to modern VLA frameworks.
    - Organizes existing methods into two principal paradigms: End-to-End VLA and Dual-System VLA, further distinguishing subclasses like textual vs. numerical action generators.
    - Summarizes representative datasets and benchmarks for evaluating VLA-based driving systems and highlights key challenges including robustness, interpretability, and instruction fidelity.

---


- [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
  - Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Hongwei Xie, Bing Wang, Guang Chen, Dingkang Liang, Xiang Bai
  - Publish Date: 2025.12.15
  - Task: End-to-End
  - Datasets: [Bench2Drive](https://bench2drive.github.io/)
  - SummaryÔºö
    - MindDrive, a Vision-Language-Action (VLA) framework for autonomous driving using online reinforcement learning to address limitations of Imitation Learning like distribution shift.
    - It employs a single LLM with two LoRA parameter sets: a Decision Expert for reasoning and an Action Expert to map decisions to trajectories, enabling efficient trial-and-error learning over discrete linguistic decisions.
    - Achieves a Driving Score of 78.04 and Success Rate of 55.09% on the Bench2Drive benchmark using the lightweight Qwen-0.5B LLM, demonstrating the first effective online RL application for VLA in driving.

---


- [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
  - Zhe Liu, Runhui Huang, Rui Yang, Siming Yan, Zining Wang, Lu Hou, Di Lin, Xiang Bai, Hengshuang Zhao
  - Publisher: Huazhong University of Science and Technology
  - Publish Date: 2025.12.14
  - Code: [DrivePI](https://github.com/happinesslz/DrivePI)
  - Task: Planning
  - Datasets: [nuScenes](https://www.nuscenes.org/)
  - SummaryÔºö
    - DrivePI, a novel spatial-aware 4D MLLM serving as a unified Vision-Language-Action (VLA) framework for autonomous driving, integrating point clouds, multi-view images, and language.
    - The method jointly performs spatial understanding, 3D perception (occupancy), prediction (occupancy flow), and planning through end-to-end optimization.
    - With only a 0.5B parameter backbone, DrivePI matches or exceeds both existing VLA models and specialized vision-action models on key benchmarks like nuScenes and OpenOcc.

---


- [Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226)
  - Shuhan Tan, Kashyap Chitta, Yuxiao Chen, Ran Tian, Yurong You, Yan Wang, Wenjie Luo, Yulong Cao, Philipp Krahenbuhl, Marco Pavone, Boris Ivanovic
  - Publisher: University of Texas at Austin, NVIDIA, Stanford University, University of Toronto
  - Publish Date: 2025.12.11
  - Task: End-to-End
  - SummaryÔºö
    - Presents Latent-CoT-Drive (LCDrive), a model that expresses chain-of-thought reasoning in a latent language aligned with action outcomes, rather than natural language.
    - Unifies reasoning and decision-making by interleaving action-proposal tokens and world model tokens grounded in a learned latent world model.
    - Achieves faster inference and better trajectory quality on a large-scale end-to-end driving benchmark compared to non-reasoning and text-reasoning baselines.

---


- [UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving](https://arxiv.org/abs/2512.09864)
  - Hao Lu, Ziyang Liu, Guangfeng Jiang, Yuanfei Luo, Sheng Chen, Yangang Zhang, Ying-Cong Chen
  - Publish Date: 2025.12.10
  - Task: End-to-End
  - SummaryÔºö
    - UniUGP, a unified Understanding-Generation-Planning framework that synergizes scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture.
    - It integrates pre-trained VLMs and video generation models to leverage visual dynamics and semantic reasoning for enhanced planning performance.
    - Introduces a four-stage training strategy across multiple AD datasets and proposed specialized datasets, demonstrating state-of-the-art performance and generalization to long-tail scenarios.

---


- [WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2512.11872)
  - Mingwang Xu, Jiahao Cui, Feipeng Cai, Hanlin Shang, Zhihao Zhu, Shan Luan, Yifang Xu, Neng Zhang, Yaoyi Li, Jia Cai, Siyu Zhu
  - Publisher: Fudan University
  - Publish Date: 2025.12.06
  - Code: [WAM-Diff](https://github.com/fudan-generative-vision/WAM-Diff)
  - Task: Planning
  - Datasets: [NAVSIM](https://github.com/autonomousvision/navsim)
  - SummaryÔºö
    - WAM-Diff, a Vision-Language-Action (VLA) framework that employs masked diffusion to iteratively refine a discrete sequence for future ego-trajectory generation in autonomous driving.
    - Features three key innovations: adaptation of masked diffusion for flexible decoding, scalable capacity via a sparse Mixture of Experts (MoE) architecture, and online reinforcement learning using Group Sequence Policy Optimization (GSPO).
    - Achieves state-of-the-art performance on NAVSIM benchmarks, demonstrating masked diffusion as a promising alternative to autoregressive and continuous diffusion policies for trajectory generation.

---


- [WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving](https://arxiv.org/abs/2512.06112)
  - Yifang Xu, Jiahao Cui, Feipeng Cai, Zhihao Zhu, Hanlin Shang, Shan Luan, Mingwang Xu, Neng Zhang, Yaoyi Li, Jia Cai, Siyu Zhu
  - Publish Date: 2025.12.05
  - Task: Planning
  - Datasets: [NAVSIM](https://github.com/autonomousvision/navsim)
  - SummaryÔºö
    - WAM-Flow, a vision-language-action (VLA) model for ego-trajectory planning, which frames planning as discrete flow matching over a structured token space, enabling fully parallel, bidirectional denoising.
    - The approach features a multi-stage adaptation that converts a pre-trained auto-regressive backbone into a non-causal flow model, enhanced by a geometry-aware flow objective and simulator-guided GRPO alignment for safety and comfort.
    - The model achieves state-of-the-art closed-loop performance on the NAVSIM benchmark, establishing discrete flow matching as a promising paradigm for end-to-end autonomous driving.

---
