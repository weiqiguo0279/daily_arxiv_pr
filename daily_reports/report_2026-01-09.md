# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì

**Êó•Êúü**: 2026-01-09
**ËÆ∫ÊñáÊï∞Èáè**: 5 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


**ÊÄªÁªì**:
- [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
  - Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo
  - Publish Date: 2025.10.06
  - Task: Reasoning, Planning
  - Datasets: [nuPlan](https://www.nuscenes.org/nuplan)
  - SummaryÔºö
    - Introduces DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT) automatically generated from nuPlan, enabling clean information ablations to study reasoning-planning causality.
    - Presents evidence for a Reasoning-Planning Decoupling Hypothesis, showing planning in trained VLM agents primarily relies on priors rather than the generated reasoning, which appears as an ancillary byproduct.
    - Proposes a novel, training-free diagnostic probe to measure an agent's reliance on priors by evaluating planning robustness against minor input perturbations.

---


**ÊÄªÁªì**:
- [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
  - Tim Kreuzer, Jelena Zdravkovic, Panagiotis Papapetrou
  - Publisher: Stockholm University
  - Publish Date: 2025.08.26
  - Task: Prediction
  - SummaryÔºö
    - PAX-TS, a model-agnostic post-hoc algorithm for explaining time series forecasting models via localized input perturbations, providing multi-granular explanations.
    - The method characterizes cross-channel correlations for multivariate forecasts and identifies distinct explanation patterns that correlate with model performance.

---


**ÊÄªÁªì**:
- [PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography](https://arxiv.org/abs/2508.04062)
  - Yichi Zhang, Wenbo Zhang, Zehui Ling, Gang Feng, Sisi Peng, Deshu Chen, Yuchen Liu, Hongwei Zhang, Shuqi Wang, Lanlan Li, Limei Han, Yuan Cheng, Zixin Hu, Yuan Qi, Le Xue
  - Publish Date: 2025.08.06
  - Task: VQA
  - Datasets: [PET2Rep](https://arxiv.org/abs/2508.04062)
  - SummaryÔºö
    - Introduces PET2Rep, a large-scale benchmark for evaluating general and medical Vision-Language Models (VLMs) on radiology report generation for PET images, addressing a gap in existing benchmarks focused on structural imaging.
    - The benchmark includes clinical efficacy metrics for evaluating radiotracer uptake descriptions and conducts a comparison of 30 state-of-the-art VLMs, finding current models perform poorly for practical PET report generation.
    - Identifies key insufficiencies in current VLMs for this medical application to guide future development.

---


**ÊÄªÁªì**:
- [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
  - Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak
  - Publish Date: 2025.07.21
  - Code: [diffusion-scaling](https://diffusion-scaling.github.io)
  - Task: Reasoning
  - SummaryÔºö
    - Systematically studies masked diffusion language models in data-constrained settings, finding they significantly outperform autoregressive models when compute is abundant but data is scarce.
    - Derives new scaling laws for diffusion models and a closed-form expression for the critical compute threshold where diffusion begins to outperform autoregressive modeling.
    - Explains that diffusion models excel due to their randomized masking objective, which acts as an implicit data augmentation over token orderings that autoregressive models lack.

---


**ÊÄªÁªì**:
- [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/abs/2505.19381)
  - Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuweng Heng, Hao Jiang, Yunda Dong, Zongzheng Zhang, Xianda Guo, Hao Sun, Hao Zhao
  - Publish Date: 2025.05.26
  - Task: Planning
  - SummaryÔºö
    - DiffVLA, a novel hybrid sparse-dense diffusion policy for autonomous driving, empowered by a Vision-Language Model (VLM) to guide trajectory generation.
    - The method explores sparse diffusion for efficient multi-modal behavior and improves planning through deep interaction across agents, map instances, and VLM outputs.
    - Achieves superior performance in the Autonomous Grand Challenge 2025, scoring 45.0 PDMS in challenging real and reactive synthetic scenarios.

---
