# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì(LLM4AD/VLM4AD/VLA4AD)

**Êó•Êúü**: 2026-01-10
**ËÆ∫ÊñáÊï∞Èáè**: 4 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


- [ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving](https://arxiv.org/abs/2601.04714)
  - Chang Zhao, Zheming Yang, Yunqing Hu, Qi Guo, Zijian Wang, Pengcheng Li, Wen Ji
  - Publish Date: 2026.01.08
  - Task: Planning
  - SummaryÔºö
    - ThinkDrive, a Chain-of-Thought (CoT) guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization.
    - The method employs a two-stage training strategy: first performing SFT using CoT explanations, then applying progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity.
    - Evaluation on a public dataset shows ThinkDrive outperforms strong RL baselines and a 2B-parameter model trained with the method surpasses the much larger GPT-4o by 3.28% on the exam metric.

---


- [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
  - Zhexiao Xiong, Xin Ye, Burhan Yaman, Sheng Cheng, Yiren Lu, Jingru Luo, Nathan Jacobs, Liu Ren
  - Publish Date: 2026.01.07
  - Project Page: [UniDrive-WM](https://unidrive-wm.github.io/UniDrive-WM)
  - Task: Planning
  - Datasets: [Bench2Drive](https://github.com/autonomousvision/bench2drive)
  - SummaryÔºö
    - UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture.
    - The model's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames, providing supervisory signals that enhance understanding and iteratively refine trajectory generation.
    - Experiments on Bench2Drive show UniDrive-WM improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method, while producing high-fidelity future images.

---


- [A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving](https://arxiv.org/abs/2601.03519)
  - Liangdong Zhang, Yiming Nie, Haoyang Li, Fanjie Kong, Baobao Zhang, Shunxin Huang, Kai Fu, Chen Min, Liang Xiao
  - Publish Date: 2026.01.07
  - Task: Planning
  - Datasets: [RELLIS-3D](https://github.com/unmannedlab/RELLIS-3D)
  - SummaryÔºö
    - Proposes OFF-EMMA, an end-to-end multimodal framework for off-road autonomous driving, addressing insufficient spatial perception and unstable reasoning in VLA models.
    - Introduces a visual prompt block using semantic segmentation masks to enhance spatial understanding and a chain-of-thought with self-consistency reasoning strategy to improve planning robustness.

---


- [FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder](https://arxiv.org/abs/2601.03460)
  - Zeyu Dong, Yimin Zhu, Yu Wu, Yu Sun
  - Publish Date: 2026.01.06
  - Task: End-to-End
  - Datasets: [Waymo Open E2E Dataset](https://waymo.com/open/)
  - SummaryÔºö
    - FROST-Drive, a novel End-to-End (E2E) architecture that preserves the generalization capabilities of a pretrained Vision-Language Model (VLM) by keeping its vision encoder frozen.
    - The model combines the frozen encoder with a transformer-based adapter and a GRU-based decoder for waypoint generation, and introduces a custom loss function to optimize for Rater Feedback Score (RFS).

---
