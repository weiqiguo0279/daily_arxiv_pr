# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì(LLM4AD/VLM4AD/VLA4AD)

**Êó•Êúü**: 2026-01-11
**ËÆ∫ÊñáÊï∞Èáè**: 4 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


- [ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving](https://arxiv.org/abs/2601.04714)
  - Chang Zhao, Zheming Yang, Yunqing Hu, Qi Guo, Zijian Wang, Pengcheng Li, Wen Ji
  - Publish Date: 2026.01.08
  - Task: Planning
  - SummaryÔºö
    - ThinkDrive, a Chain-of-Thought (CoT) guided progressive reinforcement learning fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization.
    - The method employs a two-stage training strategy: first performing supervised fine-tuning (SFT) using CoT explanations, then applying progressive RL with a difficulty-aware adaptive policy optimizer.
    - Evaluation on a public dataset shows ThinkDrive outperforms strong RL baselines and a 2B-parameter model trained with this method surpasses the much larger GPT-4o on the exam metric.
    - ThinkDrive, a Chain-of-Thought (CoT) guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization.
    - Employs a two-stage training strategy: first SFT using CoT explanations, then progressive RL with a difficulty-aware adaptive policy optimizer that adjusts learning intensity based on sample complexity.
    - Outperforms strong RL baselines on key metrics and shows a 2B-parameter model trained with this method surpasses GPT-4o by 3.28% on the exam metric.

---


- [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
  - Zhexiao Xiong, Xin Ye, Burhan Yaman, Sheng Cheng, Yiren Lu, Jingru Luo, Nathan Jacobs, Liu Ren
  - Publisher: Washington University in St. Louis, Bosch Research
  - Publish Date: 2026.01.07
  - Project Page: [UniDrive-WM](https://unidrive-wm.github.io/UniDrive-WM)
  - Task: Planning
  - Datasets: [Bench2Drive](https://bench2drive.github.io/)
  - SummaryÔºö
    - UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture.
    - The model's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames, providing supervisory signals that enhance understanding and refine trajectory generation.
    - Experiments on Bench2Drive show UniDrive-WM improves planning by 5.9% in L2 error and 9.2% in collision rate over prior methods, demonstrating advantages of integrating VLM reasoning, planning, and generative modeling.
    - The model's trajectory planner predicts a future trajectory to condition a VLM-based image generator, producing future frames that provide supervisory signals to enhance understanding and iteratively refine trajectory generation.
    - Experiments on Bench2Drive show improvements of 5.9% in L2 trajectory error and 9.2% in collision rate over prior methods, demonstrating advantages of integrating VLM reasoning, planning, and generative world modeling.

---


- [A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving](https://arxiv.org/abs/2601.03519)
  - Liangdong Zhang, Yiming Nie, Haoyang Li, Fanjie Kong, Baobao Zhang, Shunxin Huang, Kai Fu, Chen Min, Liang Xiao
  - Publish Date: 2026.01.07
  - Task: Planning
  - Datasets: [RELLIS-3D](https://github.com/unmannedlab/RELLIS-3D)
  - SummaryÔºö
    - Proposes OFF-EMMA, an end-to-end multimodal framework for off-road autonomous driving, addressing insufficient spatial perception and unstable reasoning in VLA models.
    - Introduces a visual prompt block using semantic segmentation masks to enhance spatial understanding and a chain-of-thought with self-consistency (COT-SC) reasoning strategy for robust trajectory planning.
    - Proposes OFF-EMMA, an end-to-end multimodal framework for off-road autonomous driving, designed to overcome insufficient spatial perception and unstable reasoning in VLA models.
    - Introduces a visual prompt block using semantic segmentation masks to enhance spatial understanding and a chain-of-thought with self-consistency (COT-SC) reasoning strategy to improve planning robustness.

---


- [FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder](https://arxiv.org/abs/2601.03460)
  - Zeyu Dong, Yimin Zhu, Yu Wu, Yu Sun
  - Publish Date: 2026.01.06
  - Task: End-to-End
  - Datasets: [Waymo Open E2E Dataset](https://waymo.com/open/)
  - SummaryÔºö
    - FROST-Drive, a novel End-to-End (E2E) architecture that preserves the generalization of a pretrained Vision-Language Model (VLM) by keeping its vision encoder frozen for autonomous driving.
    - The model combines the frozen encoder with a transformer-based adapter and a GRU-based decoder for waypoint generation, and introduces a custom loss to optimize for Rater Feedback Score (RFS).
    - Experiments on the Waymo Open E2E Dataset show the frozen-encoder approach outperforms full fine-tuning, offering a new pathway for robust, generalizable vision-based driving models.

---
