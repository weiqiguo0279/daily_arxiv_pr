# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì(LLM4AD/VLM4AD/VLA4AD)

**Êó•Êúü**: 2026-01-14
**ËÆ∫ÊñáÊï∞Èáè**: 2 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


- [SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning](https://arxiv.org/abs/2601.08617)
  - Leo Fillioux, Omprakash Chakraborty, Ismail Ben Ayed, Paul-Henry Courn√®de, Stergios Christodoulidis, Maria Vakalopoulou, Jose Dolz
  - Publish Date: 2026.01.13
  - Task: Reasoning
  - SummaryÔºö
    - Proposes Semantic Orthogonal Calibration (SoC), a Huber-based regularizer for test-time prompt tuning of vision-language models that improves uncertainty calibration.
    - Theoretically and empirically shows that prior fully orthogonal constraints can degrade calibration by pushing semantically related classes apart, making models overconfident.
    - Demonstrates that SoC enforces smooth prototype separation while preserving semantic proximity, leading to better calibration while maintaining competitive discriminative performance.
  - Task: Perception
  - SummaryÔºö
    - Proposes Semantic Orthogonal Calibration (SoC), a Huber-based regularizer for test-time prompt tuning of vision-language models to improve uncertainty calibration.
    - SoC enforces smooth prototype separation while preserving semantic proximity, addressing overconfidence issues from fully orthogonal constraints.
    - Demonstrates improved calibration performance while maintaining competitive discriminative capabilities across comprehensive empirical validation.

---


- [Semantic Misalignment in Vision-Language Models under Perceptual Degradation](https://arxiv.org/abs/2601.08355)
  - Guo Cheng
  - Publisher: (Inferred from context: Autonomous driving / Embodied AI research)
  - Publish Date: 2026.01.13
  - Task: Perception, Reasoning
  - Publish Date: 2026.01.13
  - Task: Perception
  - Datasets: [Cityscapes](https://www.cityscapes-dataset.com/)
  - SummaryÔºö
    - Systematically studies semantic misalignment in Vision-Language Models (VLMs) under controlled degradation of upstream visual perception, using semantic segmentation as a representative module.
    - Introduces perception-realistic corruptions that cause moderate drops in segmentation metrics but lead to severe VLM failures like hallucinations, critical omissions, and inconsistent safety judgments.
    - Proposes language-level misalignment metrics to quantify these effects, revealing a disconnect between pixel-level robustness and multimodal semantic reliability in safety-critical applications.

---
