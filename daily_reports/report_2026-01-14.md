# ğŸ“š æ¯æ—¥ arXiv è®ºæ–‡æ€»ç»“(LLM4AD/VLM4AD/VLA4AD)

**æ—¥æœŸ**: 2026-01-14
**è®ºæ–‡æ•°é‡**: 2 ç¯‡
**LLM**: DeepSeek (deepseek-chat)

---


- [SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning](https://arxiv.org/abs/2601.08617)
  - Leo Fillioux, Omprakash Chakraborty, Ismail Ben Ayed, Paul-Henry CournÃ¨de, Stergios Christodoulidis, Maria Vakalopoulou, Jose Dolz
  - Publish Date: 2026.01.13
  - Task: Reasoning
  - Summaryï¼š
    - Proposes Semantic Orthogonal Calibration (SoC), a Huber-based regularizer for test-time prompt tuning of vision-language models that improves uncertainty calibration.
    - Theoretically and empirically shows that prior fully orthogonal constraints can degrade calibration by pushing semantically related classes apart, making models overconfident.
    - Demonstrates that SoC enforces smooth prototype separation while preserving semantic proximity, leading to better calibration while maintaining competitive discriminative performance.

---


- [Semantic Misalignment in Vision-Language Models under Perceptual Degradation](https://arxiv.org/abs/2601.08355)
  - Guo Cheng
  - Publisher: (Inferred from context: Autonomous driving / Embodied AI research)
  - Publish Date: 2026.01.13
  - Task: Perception, Reasoning
  - Datasets: [Cityscapes](https://www.cityscapes-dataset.com/)
  - Summaryï¼š
    - Systematically studies semantic misalignment in Vision-Language Models (VLMs) under controlled degradation of upstream visual perception, using semantic segmentation as a representative module.
    - Introduces perception-realistic corruptions that cause moderate drops in segmentation metrics but lead to severe VLM failures like hallucinations, critical omissions, and inconsistent safety judgments.
    - Proposes language-level misalignment metrics to quantify these effects, revealing a disconnect between pixel-level robustness and multimodal semantic reliability in safety-critical applications.

---
