# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì(LLM4AD/VLM4AD/VLA4AD)

**Êó•Êúü**: 2026-01-15
**ËÆ∫ÊñáÊï∞Èáè**: 7 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


- [SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning](https://arxiv.org/abs/2601.08617)
  - Leo Fillioux, Omprakash Chakraborty, Ismail Ben Ayed, Paul-Henry Courn√®de, Stergios Christodoulidis, Maria Vakalopoulou, Jose Dolz
  - Publish Date: 2026.01.13
  - Task: Perception
  - SummaryÔºö
    - Proposes Semantic Orthogonal Calibration (SoC), a Huber-based regularizer for test-time prompt tuning of vision-language models to improve uncertainty calibration.
    - Theoretically and empirically shows that prior full orthogonality constraints can degrade calibration by pushing semantically related classes apart, leading to overconfidence.
    - Demonstrates that SoC enforces smooth prototype separation while preserving semantic proximity, consistently improving calibration while maintaining competitive discriminative performance.

---


- [Semantic Misalignment in Vision-Language Models under Perceptual Degradation](https://arxiv.org/abs/2601.08355)
  - Guo Cheng
  - Publish Date: 2026.01.13
  - Task: Perception
  - Datasets: [Cityscapes](https://www.cityscapes-dataset.com/)
  - SummaryÔºö
    - Systematically studies semantic misalignment in Vision-Language Models (VLMs) under controlled degradation of upstream visual perception, using semantic segmentation on Cityscapes as a representative module.
    - Introduces perception-realistic corruptions that cause moderate drops in segmentation metrics but lead to severe VLM failures like hallucinated objects, omission of safety-critical entities, and inconsistent safety judgments.
    - Proposes language-level misalignment metrics to quantify hallucination, critical omission, and safety misinterpretation, revealing a disconnect between pixel-level robustness and multimodal semantic reliability.

---


- [Efficient Visual Question Answering Pipeline for Autonomous Driving via Scene Region Compression](https://arxiv.org/abs/2601.07092)
  - Yuliang Cai, Dongqiangzi Ye, Zitian Chen, Chongruo Wu
  - Publish Date: 2026.01.11
  - Task: VQA
  - SummaryÔºö
    - Proposes SRC-Pipeline, an efficient VLM framework for autonomous driving VQA that compresses early frame tokens into a small number of high-level tokens while keeping full tokens for recent frames.
    - Achieves a 66% reduction in FLOPs while maintaining comparable performance, enabling more effective real-time operation in safety-critical driving settings.

---


- [SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning](https://arxiv.org/abs/2601.06474)
  - Chenxu Dang, Jie Wang, Guang Li, Zhiwen Hou, Zihan You, Hangjun Ye, Jie Ma, Long Chen, Yan Wang
  - Publish Date: 2026.01.10
  - Task: Planning
  - Datasets: [nuScenes](https://www.nuscenes.org/)
  - SummaryÔºö
    - Proposes SparseOccVLA, a vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning using sparse occupancy queries as a bridge between vision and language.
    - Introduces an LLM-guided Anchor-Diffusion Planner with decoupled anchor scoring and denoising, and cross-model trajectory-condition fusion.
    - Achieves state-of-the-art results on OmniDrive-nuScenes and Occ3D-nuScenes benchmarks, and sets a new state-of-the-art for open-loop planning on the nuScenes benchmark.

---


- [Modular Autonomy with Conversational Interaction: An LLM-driven Framework for Decision Making in Autonomous Driving](https://arxiv.org/abs/2601.05806)
  - Marvin Seegert, Korbinian Moller, Johannes Betz
  - Publisher: Technical University of Munich
  - Publish Date: 2026.01.09
  - Task: Planning
  - SummaryÔºö
    - Proposes an LLM-driven framework that integrates a natural language interaction layer with the modular Autoware software stack for autonomous driving.
    - Introduces a three-component methodology: a taxonomization of interaction categories, an application-centric Domain Specific Language (DSL) for command translation, and a safety-preserving validation layer.
    - Employs a two-stage LLM architecture to ensure high transparency and provide feedback, with evaluation confirming timing efficiency and translation robustness.

---


- [SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving](https://arxiv.org/abs/2601.05640)
  - Jingyu Li, Junjie Wu, Dongnan Hu, Xiangkai Huang, Bin Sun, Zhihui Hao, Xianpeng Lang, Xiatian Zhu, Li Zhang
  - Publish Date: 2026.01.09
  - Task: Planning
  - Datasets: [NAVSIM](https://github.com/autonomousvision/navsim)
  - SummaryÔºö
    - SGDrive, a novel framework that structures Vision-Language Model (VLM) representation learning around a driving-specific scene-agent-goal hierarchy to mirror human driving cognition.
    - The framework decomposes driving understanding to provide structured spatial-temporal representations that generalist VLMs lack, integrating multi-level information for trajectory planning.
    - Achieves state-of-the-art performance among camera-only methods on the NAVSIM benchmark, validating the effectiveness of hierarchical knowledge structuring for adapting VLMs to autonomous driving.

---


- [LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction](https://arxiv.org/abs/2601.05611)
  - Chengen Xie, Bin Sun, Tianyu Li, Junjie Wu, Zhihui Hao, XianPeng Lang, Hongyang Li
  - Publish Date: 2026.01.09
  - Task: End-to-End
  - Datasets: [NAVSIM](https://github.com/autonomousvision/navsim), [nuScenes](https://www.nuscenes.org/)
  - SummaryÔºö
    - LatentVLA, a novel framework using self-supervised latent action prediction to train Vision-Language-Action (VLA) models without language annotations, eliminating linguistic bias.
    - It transfers VLA generalization to efficient vision-based networks via knowledge distillation, achieving robust performance and real-time efficiency.
    - Establishes a new state-of-the-art on the NAVSIM benchmark (92.4 PDMS) and demonstrates strong zero-shot generalization on nuScenes.

---
