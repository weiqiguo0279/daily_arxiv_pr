# ğŸ“š æ¯æ—¥ arXiv è®ºæ–‡æ€»ç»“

**æ—¥æœŸ**: 2026-01-09
**è®ºæ–‡æ•°é‡**: 2 ç¯‡
**LLM**: DeepSeek (deepseek-chat)

---


## 1. Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions

**ä½œè€…**: Abhishek Rath

**ç±»åˆ«**: cs.AI

**é“¾æ¥**: [2601.04170v1](https://arxiv.org/pdf/2601.04170v1)

**æ€»ç»“**:
- [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
  - Abhishek Rath
  - Publish Date: 2026.01.07
  - Task: Reasoning
  - Summaryï¼š
    - Introduces the concept of "agent drift," defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences in multi-agent LLM systems.
    - Proposes the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency and inter-agent agreement.
    - Proposes three mitigation strategiesâ€”episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoringâ€”to reduce drift-related errors in production agentic AI systems.

---


## 2. ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models

**ä½œè€…**: Nikhil Anand, Shwetha Somasundaram, Anirudh Phukan et al.

**ç±»åˆ«**: cs.CL, cs.AI, cs.LG

**é“¾æ¥**: [2601.04131v1](https://arxiv.org/pdf/2601.04131v1)

**æ€»ç»“**:
- [ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models](https://arxiv.org/abs/2601.04131)
  - Nikhil Anand, Shwetha Somasundaram, Anirudh Phukan, Apoorv Saxena, Koyel Mukherjee
  - Publish Date: 2026.01.07
  - Task: Reasoning
  - Datasets: [ConFiQA](https://github.com/amazon-science/context-faithful-qa)
  - Summaryï¼š
    - ContextFocus, a lightweight activation steering approach that improves context faithfulness in knowledge-conflict settings while preserving fluency and efficiency, requiring no model finetuning.
    - Evaluated on the ConFiQA benchmark against baselines like ContextDPO and COIECD, showing significant improvements in contextual-faithfulness, complementarity to prompting, and effectiveness on larger models.

---
