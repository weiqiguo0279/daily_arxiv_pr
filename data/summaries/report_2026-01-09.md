# 📚 每日 arXiv 论文总结

**日期**: 2026-01-09
**论文数量**: 5 篇
**LLM**: DeepSeek (deepseek-chat)

---


## 1. ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models

**作者**: Nikhil Anand, Shwetha Somasundaram, Anirudh Phukan et al.

**类别**: cs.CL, cs.AI, cs.LG

**链接**: [2601.04131v1](https://arxiv.org/pdf/2601.04131v1)

**总结**:
本文提出了一种名为ContextFocus的轻量级激活引导方法，旨在提升大语言模型在知识冲突情境下对外部检索上下文的忠实遵循能力。该方法无需微调模型，仅通过调整前向传播过程中的激活向量，即可有效减少模型对内部记忆知识的依赖，从而生成更忠实于给定上下文的输出，同时保持回答的流畅性和推理效率。实验表明，ContextFocus在ConFiQA基准测试中显著优于现有基线方法，且能与提示策略互补，在更大模型上仍保持鲁棒性和高效性。

---


## 2. FOREVER: Forgetting Curve-Inspired Memory Replay for Language Model Continual Learning

**作者**: Yujie Feng, Hao Wang, Jian Li et al.

**类别**: cs.LG, cs.AI, cs.CL

**链接**: [2601.03938v1](https://arxiv.org/pdf/2601.03938v1)

**总结**:
本文提出FOREVER框架，用于缓解大语言模型在持续学习中的灾难性遗忘问题。其核心创新在于借鉴艾宾浩斯人类遗忘曲线，提出以模型优化器更新幅度定义的“模型时间”来替代传统固定步长，使回放调度与模型内部学习进度对齐。该方法结合基于遗忘曲线的回放调度器和强度感知正则化机制，在多个基准测试和不同规模模型上均有效减轻了遗忘，提升了持续学习的适应性。

---


## 3. Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training

**作者**: Chi Liu, Xin Chen

**类别**: cs.LG, cs.AI, cs.CL

**链接**: [2601.03895v1](https://arxiv.org/pdf/2601.03895v1)

**总结**:
本文提出了一种改进的强化学习算法——自适应边界裁剪GRPO（ABC-GRPO），用于提升大语言模型在策略优化中的训练稳定性与泛化能力。该方法通过引入非对称且自适应的边界裁剪机制，克服了原始GRPO算法在某些场景下裁剪策略的局限性，从而增强了训练的灵活性和泛化性。实验表明，ABC-GRPO在数学推理任务上显著优于标准GRPO，并在训练过程中保持更高的熵值，有效维持了模型的探索能力，避免了过早收敛问题。

---


## 4. From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs

**作者**: Usha Shrestha, Dmitry Ignatov, Radu Timofte

**类别**: cs.CV, cs.LG

**链接**: [2601.03808v1](https://arxiv.org/pdf/2601.03808v1)

**总结**:
该论文提出了一种基于性能反馈的闭环方法，使大语言模型能够自主设计最优的数据增强变换，从而减少对启发式或暴力搜索的依赖。核心创新在于通过微调大语言模型，使其仅依据下游模型准确率这一非文本性能指标来学习变换效果，无需强化学习或奖励模型。实验表明，该方法在保持峰值精度的同时，将候选变换评估量降低至暴力搜索的1/600，并促使模型从随机生成转向任务对齐的语义设计。

---


## 5. EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning

**作者**: Jing-Cheng Pang, Liu Sun, Chang Zhou et al.

**类别**: cs.LG

**链接**: [2601.03725v1](https://arxiv.org/pdf/2601.03725v1)

**总结**:
本文提出EDCO框架，用于领域特定大语言模型（LLM）微调中的动态课程编排。其核心创新在于引入推理熵作为样本选择标准，并设计了一个可动态调整的课程学习策略，以替代传统静态课程方法。EDCO通过高效的熵估计器、基于熵的课程生成器与模型训练器协同工作，在通信、医疗和法律领域的实验中显著提升了Qwen3-4B与Llama3.2-3B模型的微调效果，同时将熵计算时间降低了83.5%。

---
