# üìö ÊØèÊó• arXiv ËÆ∫ÊñáÊÄªÁªì

**Êó•Êúü**: 2026-01-09
**ËÆ∫ÊñáÊï∞Èáè**: 5 ÁØá
**LLM**: DeepSeek (deepseek-chat)

---


**ÊÄªÁªì**:
- [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
  - Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo
  - Publish Date: 2025.10.06
  - Task: Planning
  - Datasets: [nuPlan](https://www.nuscenes.org/nuplan)
  - SummaryÔºö
    - Introduces DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT) automatically generated from nuPlan to investigate the causal link between reasoning and planning in VLM driving agents.
    - Presents evidence of a consistent causal disconnect between reasoning and planning, proposing the Reasoning-Planning Decoupling Hypothesis where reasoning is an ancillary byproduct rather than a causal mediator.
    - Proposes a novel, training-free diagnostic probe to measure an agent's reliance on priors by evaluating planning robustness against minor input perturbations.

---


**ÊÄªÁªì**:
- [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
  - Tim Kreuzer, Jelena Zdravkovic, Panagiotis Papapetrou
  - Publisher: Stockholm University
  - Publish Date: 2025.08.26
  - Task: Prediction
  - SummaryÔºö
    - PAX-TS, a model-agnostic post-hoc algorithm to explain time series forecasting models via localized input perturbations, producing multi-granular explanations and characterizing cross-channel correlations for multivariate forecasts.
    - The method's explanations effectively capture model behavior, with identified explanation patterns serving as indicators of forecasting performance across different datasets and algorithms.

---


**ÊÄªÁªì**:
- [PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography](https://arxiv.org/abs/2508.04062)
  - Yichi Zhang, Wenbo Zhang, Zehui Ling, Gang Feng, Sisi Peng, Deshu Chen, Yuchen Liu, Hongwei Zhang, Shuqi Wang, Lanlan Li, Limei Han, Yuan Cheng, Zixin Hu, Yuan Qi, Le Xue
  - Publish Date: 2025.08.06
  - Task: VQA
  - Datasets: [PET2Rep](https://arxiv.org/abs/2508.04062)
  - SummaryÔºö
    - Introduces PET2Rep, a large-scale benchmark for evaluating general and medical VLMs on radiology report generation for PET images, filling a gap for molecular imaging.
    - Proposes novel clinical efficacy metrics to evaluate radiotracer uptake descriptions and conducts a comprehensive comparison of 30 state-of-the-art VLMs.
    - Findings show current VLMs perform poorly on this task, highlighting key insufficiencies that need addressing for practical medical application.

---


**ÊÄªÁªì**:
- [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
  - Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak
  - Publisher: Carnegie Mellon University
  - Publish Date: 2025.07.21
  - Code: [diffusion-scaling](https://diffusion-scaling.github.io)
  - Task: Reasoning
  - SummaryÔºö
    - Systematically studies masked diffusion language models in data-constrained settings, finding they significantly outperform autoregressive models when compute is abundant but data is scarce.
    - Derives new scaling laws for diffusion models and a closed-form expression for the critical compute threshold where diffusion outperforms autoregressive.
    - Explains that diffusion models excel due to their randomized masking objective acting as implicit data augmentation, which the fixed left-to-right factorization of autoregressive models lacks.

---


**ÊÄªÁªì**:
- [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/abs/2505.19381)
  - Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuweng Heng, Hao Jiang, Yunda Dong, Zongzheng Zhang, Xianda Guo, Hao Sun, Hao Zhao
  - Publish Date: 2025.05.26
  - Task: Planning
  - SummaryÔºö
    - Proposes Diff-VLA, a novel hybrid sparse-dense diffusion policy for autonomous driving, empowered by a Vision-Language Model (VLM).
    - Explores sparse diffusion representation for efficient multi-modal driving behavior and improves trajectory generation via deep interaction across agent, map instances, and VLM output.
    - Achieves superior performance in the Autonomous Grand Challenge 2025, with a score of 45.0 PDMS.

---
