# 📚 每日 arXiv 论文总结

**日期**: 2026-01-09
**论文数量**: 2 篇
**LLM**: DeepSeek (deepseek-chat)

---


## 1. Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions

**作者**: Abhishek Rath

**类别**: cs.AI

**链接**: [2601.04170v1](https://arxiv.org/pdf/2601.04170v1)

**总结**:
本研究提出了“智能体漂移”概念，指多智能体大语言模型系统在长期交互中行为、决策质量和协作一致性逐渐退化的问题。论文构建了理论框架，将漂移分为语义漂移、协调漂移和行为漂移三类，并设计了包含12个维度的智能体稳定性指数（ASI）进行量化评估。研究进一步提出了基于情景记忆、漂移感知路由和自适应行为锚定的缓解策略，为提升生产级AI系统的长期稳定性和可靠性提供了方法论基础。
本研究提出了“智能体漂移”概念，用于描述多智能体大语言模型系统在长期交互中行为、决策质量和协作一致性的渐进性退化。论文构建了理论框架，将漂移分为语义漂移、协调漂移和行为漂移三类，并创新性地设计了包含12个维度的智能体稳定性指数（ASI）进行量化评估。研究进一步提出了情景记忆巩固、漂移感知路由协议和自适应行为锚定三种缓解策略，为提升生产级AI系统的长期稳定性和部署可靠性提供了方法论基础。
本研究提出了“智能体漂移”概念，指多智能体大语言模型系统在长期交互中行为质量、决策一致性和协作效率逐渐退化的问题。论文构建了理论框架，将漂移分为语义漂移、协调漂移和行为漂移三类，并设计了包含12个维度的智能体稳定性指数（ASI）进行量化评估。研究进一步提出了情景记忆巩固、漂移感知路由协议和自适应行为锚定三种缓解策略，为提升生产级AI系统的长期稳定性和部署可靠性提供了方法论基础。

---


## 2. Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning

**作者**: Yifan Wang, Yanyu Li, Sergey Tulyakov et al.

**类别**: cs.CV

**链接**: [2601.04153v1](https://arxiv.org/pdf/2601.04153v1)

**总结**:
本文提出Diffusion-DRF，一种用于视频扩散模型微调的可微分奖励流方法。其核心创新在于利用冻结的视觉语言模型（VLM）作为无需训练的评估器，通过反向传播将VLM的反馈直接融入去噪链的优化过程中，从而避免依赖人工标注或可学习的奖励模型。该方法通过自动化、多维度提示获取可靠反馈，并结合梯度检查点技术实现高效更新，显著提升了生成视频的质量和语义对齐，同时有效缓解了奖励破解和训练不稳定的问题，且无需额外奖励数据即可泛化至其他扩散生成任务。
本文提出Diffusion-DRF方法，通过可微分的奖励流对视频扩散模型进行微调。该方法的核心创新在于直接利用冻结的视觉语言模型（VLM）作为无需训练的评判器，将VLM的多维度反馈通过去噪链反向传播，转化为词元感知的梯度以优化生成过程。这一设计避免了传统偏好优化方法对人工标注或可学习奖励模型的依赖，有效缓解了奖励破解和训练不稳定的问题，在提升视频质量与语义对齐的同时，无需额外奖励数据即可实现稳定高效的微调。
本文提出Diffusion-DRF，一种用于视频扩散模型微调的可微分奖励流方法。其核心创新在于利用冻结的视觉语言模型（VLM）作为无需训练的评判器，通过反向传播将VLM的反馈直接融入去噪链，从而生成面向令牌的梯度进行优化。该方法通过自动化的多维度提示流程获取可靠反馈，并结合梯度检查点技术实现高效更新，在提升视频质量与语义对齐的同时，有效避免了奖励破解和训练不稳定问题，且无需额外奖励模型或偏好数据。

---
