# ðŸš— Autonomous Driving VLM/VLA Papers

- [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
  - Yazan Obeidi, Amir Sarfi, Joel Lidin, Paul Janson, Eugene Belilovsky
  - Publish Date: 2026.01.05
  - Task: Reasoning
  - Summaryï¼š
    - Introduces a heterogeneous distributed training framework for LLM pre-training, combining low-communication data parallelism (SparseLoCo) with low-bandwidth pipeline model parallelism via activation and gradient compression.
    - Proposes grouping resource-limited participants to jointly instantiate a model replica using pipeline parallelism with subspace-projected inter-stage communication.
    - Finds that selective heterogeneous compression improves the loss-communication tradeoff, especially at aggressive compression ratios, suggesting a path to incorporating low-bandwidth model parallelism into LLM pre-training.

- [VINO: A Unified Visual Generator with Interleaved OmniModal Context](https://arxiv.org/abs/2601.02358)
  - Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye
  - Publish Date: 2026.01.05
  - Task: Generation
  - Summaryï¼š
    - VINO, a unified visual generator that performs image and video generation and editing within a single framework using a shared diffusion backbone.
    - It couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), encoding multimodal inputs as interleaved tokens to guide the diffusion process.
    - The model is trained with a multi-stage pipeline, demonstrating strong visual quality, faithful instruction following, and improved reference and attribute preservation across diverse benchmarks.

- [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
  - Sourena Khanzadeh
  - Publish Date: 2026.01.05
  - Task: Reasoning
  - Summaryï¼š
    - Introduces Project Ariadne, a novel XAI framework using Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity and faithfulness of LLM agent reasoning traces.
    - Defines and detects a failure mode termed Causal Decoupling, revealing a persistent Faithfulness Gap where reasoning traces can be unfaithful "Reasoning Theater" while decisions are governed by latent parametric priors.
    - Proposes the Ariadne Score as a new benchmark for aligning stated logic with model action, based on measuring Causal Sensitivity through hard interventions on intermediate reasoning nodes.

- [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
  - Yazan Obeidi, Amir Sarfi, Joel Lidin, Paul Janson, Eugene Belilovsky
  - Publish Date: 2026.01.05
  - Task: Reasoning
  - Summaryï¼š
    - Introduces a heterogeneous distributed training framework for LLM pre-training, combining low-communication data parallelism (SparseLoCo) with low-bandwidth pipeline model parallelism via activation and gradient compression.
    - Proposes grouping resource-limited participants to jointly instantiate a model replica using pipeline parallelism with subspace-projected inter-stage communication.
    - Finds that selective heterogeneous compression improves the loss-communication tradeoff, especially at aggressive compression ratios, suggesting a path to incorporate low-bandwidth model parallelism into LLM pre-training.

- [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
  - Yazan Obeidi, Amir Sarfi, Joel Lidin, Paul Janson, Eugene Belilovsky
  - Publish Date: 2026.01.05
  - Summaryï¼š
    - Introduces a heterogeneous distributed training framework for LLM pre-training, combining low-communication data parallelism (SparseLoCo) with low-bandwidth pipeline model parallelism via activation compression.
    - Proposes adaptations to make subspace pipeline compression compatible with SparseLoCo, enabling participation from resource-limited devices.
    - Finds that selective heterogeneous compression improves the loss-communication tradeoff, especially at aggressive compression ratios, offering a path to scalable pre-training under bandwidth constraints.

- [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
  - Yazan Obeidi, Amir Sarfi, Joel Lidin, Paul Janson, Eugene Belilovsky
  - Publish Date: 2026.01.05
  - Summaryï¼š
    - Introduces a heterogeneous distributed training framework for LLM pre-training, combining low-communication data parallelism (SparseLoCo) with low-bandwidth pipeline model parallelism via activation compression.
    - Proposes adaptations to make subspace pipeline compression compatible with SparseLoCo, finding that selective heterogeneous compression improves the loss-communication tradeoff, especially at aggressive compression ratios.


---

*Last updated: 2026-01-07 00:25:47 UTC*
*Auto-generated by ArxivVLMCrawler*