
# Daily arXiv â€“ AI Research Tracker ğŸ“šğŸ¤–

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![arXiv](https://img.shields.io/badge/arXiv-cs.AI-b31b1b.svg)](https://arxiv.org/list/cs.AI/recent)

**English Document** | [ä¸­æ–‡æ–‡æ¡£](README_zh.md)

Automatically track the latest AI research papers on arXiv each day, use LLMs for intelligent summarization, and generate research trend analysis reports.

## âœ¨ Features

### Core Functions

- ğŸ” **Intelligent Crawling**: Daily automatic fetching of the newest papers from arXiv in specified fields  
  - Supports multiple research areas (cs.AI, cs.LG, cs.CV, etc.)  
  - Keyword filtering  
  - TFâ€‘IDF based smart selection  

- ğŸ¤– **Multiâ€‘Model Summarization**: Use LLMs to generate concise paper summaries  
  - Supports 5 LLM providers: OpenAI, Gemini, Claude, DeepSeek, vLLM  
  - Bilingual (Chinese & English) summaries  
  - Concurrent processing for higher efficiency  

- ğŸ“Š **Trend Analysis**: Inâ€‘depth analysis of research hot topics and technological trends  
  - TFâ€‘IDF keyword extraction  
  - LDA topic modeling  
  - Wordâ€‘cloud visualization  
  - LLM deep analysis (research hotspots, technology trends, future directions)  

- ğŸŒ **Web Interface**: Modern responsive web UI  
  - Built with BootstrapÂ 5  
  - Realâ€‘time data display  
  - Detailed paper view  
  - Pagination and filtering  

- â° **Scheduled Execution**: Various scheduling options  
  - APScheduler (recommended)  
  - Linux cron jobs  
  - Systemd service  

- ğŸ“§ **Email Notifications**: Execution status via email  
  - Elegant HTML email templates  
  - Separate success/failure notices  
  - Detailed statistics  

## ğŸ“¸ Interface Preview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“Š Statistics Overview                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ğŸ“„ Papersâ”‚ â”‚ ğŸ“ Summariesâ”‚ â”‚ ğŸ·ï¸ Categoriesâ”‚ â”‚ ğŸ”‘ Keywordsâ”‚   â”‚
â”‚  â”‚   20    â”‚ â”‚   20    â”‚ â”‚    2    â”‚ â”‚   50   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š Trend Analysis   â”‚  ğŸ“š Paper List                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸŒ Word Cloud                                          â”‚
â”‚  [WordCloud visualization]                              â”‚
â”‚                                                          â”‚
â”‚  ğŸ”¥ Research Hotspots                                    â”‚
â”‚  â€¢ transformer (0.85)                                    â”‚
â”‚  â€¢ large language model (0.72)                           â”‚
â”‚                                                          â”‚
â”‚  ğŸ“ˆ Technology Trends                                    â”‚
â”‚  [LLMâ€‘generated analysis content]                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Paper Card                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ğŸ“„ Attention Is All You Need                       â”‚ â”‚
â”‚  â”‚ ğŸ‘¥ Authors: Vaswani et al.                         â”‚ â”‚
â”‚  â”‚ ğŸ“… 2017â€‘06â€‘12                                      â”‚ â”‚
â”‚  â”‚ ğŸ“ [Abstract preview â€¦]                            â”‚ â”‚
â”‚  â”‚ [View Details] [arXiv Original]                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- PythonÂ 3.12+
- Conda (recommended) or virtualenv
- LLM API keys (OpenAI / Gemini / Claude / DeepSeek / vLLM)

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/daily-arxiv.git
cd daily-arxiv
```

### 2. Create a virtual environment

```bash
# Using Conda (recommended)
conda create -n daily-arxiv python=3.12 -y
conda activate daily-arxiv

# Or using venv
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows
```

### 3. Install dependencies

```bash
pip install uv
uv pip install -r requirements.txt
```

### 4. Configure environment variables

```bash
# Copy the example file
cp .env.example .env

# Edit the .env file
nano .env
```

Add your API keys:

```bash
# OpenAI
OPENAI_API_KEY=sk-...

# Google Gemini
GEMINI_API_KEY=...

# Anthropic Claude
ANTHROPIC_API_KEY=...

# DeepSeek
DEEPSEEK_API_KEY=...

# vLLM (local deployment)
VLLM_API_KEY=EMPTY

# Email notifications (optional)
EMAIL_PASSWORD=your-app-password
```

### 5. Configure `config.yaml`

Edit `config/config.yaml`:

```yaml
# Research fields
arxiv:
  categories:
    - "cs.AI"  # Artificial Intelligence
    - "cs.LG"  # Machine Learning
  
  keywords:
    - "large language model"
    - "transformer"
  
  max_results: 20

# LLM provider
llm:
  provider: "vllm"  # openai, gemini, claude, deepseek, vllm

# Scheduler settings
scheduler:
  enabled: true
  run_time: "09:00"
  timezone: "Asia/Shanghai"
```

### 6. Run tests

```bash
# Test paper fetching
python test_fetcher.py

# Test LLM summarization
python test_summarizer.py

# Test trend analysis
python test_analyzer.py

# Test web service
python test_web.py

# Test scheduler
python test_scheduler.py
```

### 7. Execute the full workflow

```bash
# Manual single run
python main.py
```

### 8. Start the web service

```bash
# Development mode
python src/web/app.py

# Open http://localhost:5000
```

### 9. Launch scheduled execution

```bash
# Recommended: use the start script
./deploy/start.sh

# Or run directly
python scheduler.py
```

Visit http://localhost:5000 to view results.

## ğŸ“‚ Project Structure

```
daily-arxiv/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Main configuration file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â””â”€â”€ arxiv_fetcher.py    # arXiv paper crawler
â”‚   â”œâ”€â”€ summarizer/
â”‚   â”‚   â”œâ”€â”€ base_llm_client.py  # Base LLM class
â”‚   â”‚   â”œâ”€â”€ openai_client.py    # OpenAI client
â”‚   â”‚   â”œâ”€â”€ gemini_client.py    # Gemini client
â”‚   â”‚   â”œâ”€â”€ claude_client.py    # Claude client
â”‚   â”‚   â”œâ”€â”€ deepseek_client.py  # DeepSeek client
â”‚   â”‚   â”œâ”€â”€ vllm_client.py      # vLLM client
â”‚   â”‚   â”œâ”€â”€ llm_factory.py      # LLM factory
â”‚   â”‚   â””â”€â”€ paper_summarizer.py # Paper summarizer
â”‚   â”œâ”€â”€ analyzer/
â”‚   â”‚   â””â”€â”€ trend_analyzer.py   # Trend analysis
â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â”œâ”€â”€ app.py             # Flask web app
â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”‚       â””â”€â”€ index.html     # Web UI page
â”‚   â”œâ”€â”€ notifier/
â”‚   â”‚   â””â”€â”€ email_notifier.py  # Email notifier
â”‚   â””â”€â”€ utils.py               # Utility functions
â”œâ”€â”€ static/
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ main.js            # Frontâ€‘end JavaScript
â”œâ”€â”€ data/                      # Data storage
â”‚   â”œâ”€â”€ papers/               # Paper JSON files
â”‚   â”œâ”€â”€ summaries/            # Summary JSON files
â”‚  â”€â”€/ # wordâ€‘cloud images
â”œâ”€â”€ logs/                     # Log files
â”œâ”€â”€ deploy/                   # Deployment scripts
â”‚   â”œâ”€â”€ start.sh             # Start script
â”‚   â”œâ”€â”€ daily-arxiv.service  # Systemd service
â”‚   â””â”€â”€ crontab.example      # Cron example
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ arxiv_fetcher_guide.md
â”‚   â”œâ”€â”€ trend_analyzer_guide.md
â”‚   â”œâ”€â”€ web_interface_guide.md
â”‚   â””â”€â”€ scheduler_guide.md
â”œâ”€â”€ main.py                   # Main entry point
â”œâ”€â”€ scheduler.py              # APScheduler dispatcher
â”œâ”€â”€ test_*.py                # Test scripts
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example            # Example env file
â””â”€â”€ README.md               # Project overview
```

## âš™ï¸ Configuration Details

### arXiv Category Codes

Common Computer Science categories:  
- `cs.AI` â€“ Artificial Intelligence  
- `cs.LG` â€“ Machine Learning  
- `cs.CV` â€“ Computer Vision  
- `cs.CL` â€“ Computation and Language (NLP)  
- `cs.NE` â€“ Neural and Evolutionary Computing  
- `stat.ML` â€“ Machine Learning (Statistics)  

See the full list at: https://arxiv.org/category_taxonomy

### LLM Providers

Supported providers:  
- **OpenAI**: GPTâ€‘4, GPTâ€‘3.5â€‘turbo  
- **Gemini**: Gemini models  
- **Anthropic**: Claude  
- **DeepSeek**: DeepSeek models  
- **vLLM**: Locally run openâ€‘source models (OpenAIâ€‘compatible API)

## ğŸ“ Development Roadmap

- [x] Project scaffolding âœ…  
- [x] arXiv crawling âœ…  
- [x] LLM summarization âœ…  
  - Support OpenAI, Gemini, Claude, DeepSeek, vLLM  
- [x] Trend analysis âœ…  
  - Keyword extraction, topic modeling, wordâ€‘cloud generation  
  - LLMâ€‘driven deep analysis (hotspots, trends, innovations)  
- [x] Web UI development  
- [x] Scheduling functionality  
- [x] Testing & optimization  
- [ ] UI beautification  
- [ ] Add WeChat public account integration  

## ğŸ§ª Testing

```bash
# Test paper crawler
python test_fetcher.py

# Test summarizer
python test_summarizer.py

# Test trend analyzer
python test_analyzer.py

# Run full pipeline
python main.py
```

## ğŸ“Š Generated Files

```
data/
â”œâ”€â”€ papers/
â”‚   â”œâ”€â”€ papers_YYYY-MM-DD.json   # Daily paper data
â”‚   â””â”€â”€ latest.json              # Latest paper data
â”œâ”€â”€ summaries/
â”‚   â”œâ”€â”€ summaries_YYYY-MM-DD.json# Daily summaries
â”‚   â””â”€â”€ latest.json              # Latest summaries
â””â”€â”€ analysis/
    â”œâ”€â”€ wordcloud_YYYY-MM-DD.png # Wordâ€‘cloud image
    â”œâ”€â”€ analysis_YYYY-MM-DD.json # Analysis results
    â”œâ”€â”€ report_YYYY-MM-DD.md     # Markdown report
    â””â”€â”€ latest.json              # Latest analysis data
```

## ğŸ“– Documentation

- [Paper Crawler Guide](docs/arxiv_fetcher_guide.md)  
- [LLM Summarizer Guide](docs/llm_guide.md)  
- [Configuration Guide](docs/config_guide.md)

## ğŸ¤ Contributing

Feel free to open Issues and submit Pull Requests!

## ğŸ“„ License

MIT License