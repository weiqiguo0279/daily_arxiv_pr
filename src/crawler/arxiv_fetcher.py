"""
arXiv è®ºæ–‡çˆ¬å–å™¨

ä½¿ç”¨ arxiv API è·å–æŒ‡å®šé¢†åŸŸçš„æœ€æ–°è®ºæ–‡
"""
import arxiv
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path

from src.utils import save_json, get_date_string, get_data_path


class ArxivFetcher:
    """arXiv è®ºæ–‡çˆ¬å–å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.arxiv_config = config.get('arxiv', {})
        self.logger = logging.getLogger('daily_arxiv.fetcher')
        
        # è·å–é…ç½®
        self.categories = self.arxiv_config.get('categories', ['cs.AI'])
        self.keywords = self.arxiv_config.get('keywords', [])
        self.max_results = self.arxiv_config.get('max_results', 20)
        self.sort_by = self.arxiv_config.get('sort_by', 'submittedDate')
        self.sort_order = self.arxiv_config.get('sort_order', 'descending')
        
    def build_query(self) -> str:
        """æ„å»ºæœç´¢æŸ¥è¯¢
        
        Returns:
            æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # æ„å»ºç±»åˆ«æŸ¥è¯¢
        if len(self.categories) == 1:
            category_query = f"cat:{self.categories[0]}"
        else:
            category_parts = [f"cat:{cat}" for cat in self.categories]
            category_query = "(" + " OR ".join(category_parts) + ")"
        
        # å¦‚æœæœ‰å…³é”®è¯ï¼Œæ·»åŠ å…³é”®è¯è¿‡æ»¤
        if self.keywords:
            # æ„å»ºå…³é”®è¯æŸ¥è¯¢ï¼ˆåœ¨æ ‡é¢˜æˆ–æ‘˜è¦ä¸­æœç´¢ï¼‰
            keyword_parts = []
            for keyword in self.keywords:
                # åœ¨æ ‡é¢˜å’Œæ‘˜è¦ä¸­æœç´¢å…³é”®è¯
                keyword_parts.append(f'(ti:"{keyword}" OR abs:"{keyword}")')
            keyword_query = "(" + " OR ".join(keyword_parts) + ")"
            
            # ç»„åˆç±»åˆ«å’Œå…³é”®è¯
            query = f"{category_query} AND {keyword_query}"
        else:
            query = category_query
        
        self.logger.info(f"æ„å»ºçš„æŸ¥è¯¢: {query}")
        return query
    
    def fetch_papers(self, days_back: int = 1) -> List[Dict[str, Any]]:
        """è·å–è®ºæ–‡
        
        Args:
            days_back: è·å–è¿‡å»å‡ å¤©çš„è®ºæ–‡ï¼Œé»˜è®¤1å¤©
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹çˆ¬å– arXiv è®ºæ–‡")
        self.logger.info(f"ç±»åˆ«: {', '.join(self.categories)}")
        if self.keywords:
            self.logger.info(f"å…³é”®è¯: {', '.join(self.keywords)}")
        self.logger.info(f"æœ€å¤§ç»“æœæ•°: {self.max_results}")
        self.logger.info("=" * 60)
        
        # æ„å»ºæŸ¥è¯¢
        query = self.build_query()
        
        # è®¾ç½®æ’åºæ–¹å¼
        sort_by_map = {
            'submittedDate': arxiv.SortCriterion.SubmittedDate,
            'relevance': arxiv.SortCriterion.Relevance,
            'lastUpdatedDate': arxiv.SortCriterion.LastUpdatedDate,
        }
        sort_criterion = sort_by_map.get(self.sort_by, arxiv.SortCriterion.SubmittedDate)
        
        sort_order_map = {
            'descending': arxiv.SortOrder.Descending,
            'ascending': arxiv.SortOrder.Ascending,
        }
        sort_order = sort_order_map.get(self.sort_order, arxiv.SortOrder.Descending)
        
        # åˆ›å»ºæœç´¢å¯¹è±¡
        search = arxiv.Search(
            query=query,
            max_results=self.max_results,
            sort_by=sort_criterion,
            sort_order=sort_order
        )
        
        # è·å–è®ºæ–‡
        papers = []
        cutoff_date = datetime.now() - timedelta(days=days_back)
        
        try:
            self.logger.info("æ­£åœ¨è·å–è®ºæ–‡...")
            for result in search.results():
                # æ£€æŸ¥æäº¤æ—¥æœŸ
                if result.published.replace(tzinfo=None) < cutoff_date:
                    self.logger.debug(f"è®ºæ–‡ {result.title} å‘å¸ƒäº {result.published}ï¼Œæ—©äºæˆªæ­¢æ—¥æœŸ")
                    continue
                
                # æå–è®ºæ–‡ä¿¡æ¯
                paper = self._extract_paper_info(result)
                papers.append(paper)
                
                self.logger.info(f"âœ“ [{len(papers)}] {paper['title'][:60]}...")
            
            self.logger.info("=" * 60)
            self.logger.info(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            self.logger.info("=" * 60)
            
            # ä¿å­˜è®ºæ–‡æ•°æ®
            self._save_papers(papers)
            
            return papers
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–è®ºæ–‡å¤±è´¥: {str(e)}", exc_info=True)
            raise
    
    def _extract_paper_info(self, result: arxiv.Result) -> Dict[str, Any]:
        """æå–è®ºæ–‡ä¿¡æ¯
        
        Args:
            result: arxiv.Result å¯¹è±¡
            
        Returns:
            è®ºæ–‡ä¿¡æ¯å­—å…¸
        """
        return {
            'id': result.entry_id.split('/')[-1],  # arXiv ID
            'title': result.title,
            'authors': [author.name for author in result.authors],
            'abstract': result.summary.replace('\n', ' ').strip(),
            'categories': result.categories,
            'primary_category': result.primary_category,
            'published': result.published.isoformat(),
            'updated': result.updated.isoformat(),
            'pdf_url': result.pdf_url,
            'entry_url': result.entry_id,
            'comment': result.comment if hasattr(result, 'comment') else None,
            'journal_ref': result.journal_ref if hasattr(result, 'journal_ref') else None,
            'doi': result.doi if hasattr(result, 'doi') else None,
            'fetched_at': datetime.now().isoformat(),
        }
    
    def _save_papers(self, papers: List[Dict[str, Any]]):
        """ä¿å­˜è®ºæ–‡æ•°æ®
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
        """
        if not papers:
            self.logger.warning("æ²¡æœ‰è®ºæ–‡éœ€è¦ä¿å­˜")
            return
        
        # è·å–å­˜å‚¨è·¯å¾„
        data_path = get_data_path(self.config, 'papers')
        Path(data_path).mkdir(parents=True, exist_ok=True)
        
        # æŒ‰æ—¥æœŸä¿å­˜
        date_str = get_date_string()
        filepath = f"{data_path}/papers_{date_str}.json"
        
        # ä¿å­˜æ•°æ®
        save_json(papers, filepath)
        self.logger.info(f"ğŸ’¾ è®ºæ–‡æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        
        # åŒæ—¶ä¿å­˜ä¸€ä»½åˆ° latest.jsonï¼Œæ–¹ä¾¿ Web æœåŠ¡è¯»å–
        latest_filepath = f"{data_path}/latest.json"
        save_json({
            'date': date_str,
            'count': len(papers),
            'papers': papers
        }, latest_filepath)
        self.logger.info(f"ğŸ’¾ æœ€æ–°æ•°æ®å·²ä¿å­˜åˆ°: {latest_filepath}")
    
    def get_paper_stats(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è·å–è®ºæ–‡ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not papers:
            return {}
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        category_counts = {}
        for paper in papers:
            for category in paper['categories']:
                category_counts[category] = category_counts.get(category, 0) + 1
        
        # ç»Ÿè®¡ä½œè€…æ•°é‡
        author_counts = {}
        for paper in papers:
            for author in paper['authors']:
                author_counts[author] = author_counts.get(author, 0) + 1
        
        # æ‰¾å‡ºé«˜äº§ä½œè€…ï¼ˆå‘è¡¨2ç¯‡ä»¥ä¸Šï¼‰
        prolific_authors = {k: v for k, v in author_counts.items() if v >= 2}
        
        stats = {
            'total_papers': len(papers),
            'category_distribution': category_counts,
            'total_authors': len(author_counts),
            'prolific_authors': prolific_authors,
            'date': get_date_string(),
        }
        
        return stats
    
    def print_paper_summary(self, papers: List[Dict[str, Any]]):
        """æ‰“å°è®ºæ–‡æ‘˜è¦
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
        """
        if not papers:
            self.logger.info("æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡")
            return
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info(f"ğŸ“š ä»Šæ—¥è®ºæ–‡æ‘˜è¦ ({len(papers)} ç¯‡)")
        self.logger.info("=" * 80)
        
        for i, paper in enumerate(papers, 1):
            self.logger.info(f"\n[{i}] {paper['title']}")
            self.logger.info(f"    ä½œè€…: {', '.join(paper['authors'][:3])}" + 
                           (" et al." if len(paper['authors']) > 3 else ""))
            self.logger.info(f"    ç±»åˆ«: {', '.join(paper['categories'][:3])}")
            self.logger.info(f"    é“¾æ¥: {paper['pdf_url']}")
            self.logger.info(f"    æ‘˜è¦: {paper['abstract'][:150]}...")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = self.get_paper_stats(papers)
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        self.logger.info("=" * 80)
        self.logger.info(f"æ€»è®ºæ–‡æ•°: {stats['total_papers']}")
        self.logger.info(f"æ€»ä½œè€…æ•°: {stats['total_authors']}")
        
        if stats.get('prolific_authors'):
            self.logger.info("\né«˜äº§ä½œè€… (2ç¯‡ä»¥ä¸Š):")
            for author, count in sorted(stats['prolific_authors'].items(), 
                                       key=lambda x: x[1], reverse=True)[:5]:
                self.logger.info(f"  - {author}: {count} ç¯‡")
        
        self.logger.info("\nç±»åˆ«åˆ†å¸ƒ:")
        for category, count in sorted(stats['category_distribution'].items(), 
                                     key=lambda x: x[1], reverse=True):
            self.logger.info(f"  - {category}: {count} ç¯‡")
        
        self.logger.info("=" * 80 + "\n")


def main():
    """æµ‹è¯•å‡½æ•°"""
    from src.utils import load_config, load_env, setup_logging
    
    load_env()
    config = load_config()
    logger = setup_logging(config)
    
    fetcher = ArxivFetcher(config)
    papers = fetcher.fetch_papers(days_back=2)  # è·å–è¿‡å»2å¤©çš„è®ºæ–‡
    
    if papers:
        fetcher.print_paper_summary(papers)


if __name__ == "__main__":
    main()
