"""
è¶‹åŠ¿åˆ†æå™¨

åˆ†æè®ºæ–‡é›†åˆï¼Œç”Ÿæˆï¼š
1. è¯äº‘å›¾
2. ç ”ç©¶çƒ­ç‚¹åˆ†æ
3. è¶‹åŠ¿é¢„æµ‹
4. ç ”ç©¶åˆ›æ–°ç‚¹å»ºè®®
"""
import logging
import json
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
from collections import Counter
import re

# è¯äº‘å’Œå¯è§†åŒ–
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

# æ–‡æœ¬å¤„ç†
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import nltk
from nltk.corpus import stopwords

from src.utils import save_json, get_date_string


class TrendAnalyzer:
    """è¶‹åŠ¿åˆ†æå™¨"""
    
    def __init__(self, config: Dict[str, Any], llm_client=None):
        """åˆå§‹åŒ–
        
        Args:
            config: é…ç½®å­—å…¸
            llm_client: LLM å®¢æˆ·ç«¯ï¼ˆç”¨äºç”Ÿæˆåˆ†ææŠ¥å‘Šï¼‰
        """
        self.config = config
        self.llm_client = llm_client
        self.logger = logging.getLogger('daily_arxiv.analyzer')
        
        # ä¸‹è½½å¿…è¦çš„ NLTK æ•°æ®
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            self.logger.info("ä¸‹è½½ NLTK stopwords...")
            nltk.download('stopwords', quiet=True)
        
        self.stop_words = set(stopwords.words('english'))
        # æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯
        self.stop_words.update([
            'paper', 'study', 'research', 'approach', 'method', 'propose',
            'propose', 'present', 'show', 'demonstrate', 'arxiv', 'preprint',
            'et', 'al', 'also', 'based', 'using', 'used', 'use', 'new',
            'work', 'results', 'result', 'performance', 'model', 'models'
        ])
    
    def analyze(self, papers: List[Dict[str, Any]], summaries: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„è¶‹åŠ¿åˆ†æ
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            summaries: è®ºæ–‡æ€»ç»“åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ” å¼€å§‹è¶‹åŠ¿åˆ†æ")
        self.logger.info(f"åˆ†æè®ºæ–‡æ•°é‡: {len(papers)}")
        self.logger.info("=" * 60)
        
        if not papers:
            self.logger.warning("æ²¡æœ‰è®ºæ–‡å¯åˆ†æ")
            return {}
        
        # 1. æå–å…³é”®è¯å’Œä¸»é¢˜
        self.logger.info("\næ­¥éª¤ 1: æå–å…³é”®è¯å’Œä¸»é¢˜...")
        keywords = self._extract_keywords(papers)
        topics = self._extract_topics(papers)
        
        # 2. ç”Ÿæˆè¯äº‘
        self.logger.info("\næ­¥éª¤ 2: ç”Ÿæˆè¯äº‘...")
        wordcloud_path = self._generate_wordcloud(papers)
        
        # 3. ç»Ÿè®¡åˆ†æ
        self.logger.info("\næ­¥éª¤ 3: ç»Ÿè®¡åˆ†æ...")
        statistics = self._generate_statistics(papers, summaries)
        
        # 4. ä½¿ç”¨ LLM ç”Ÿæˆæ·±åº¦åˆ†æ
        self.logger.info("\næ­¥éª¤ 4: ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š...")
        llm_analysis = self._generate_llm_analysis(papers, summaries, keywords, topics)
        
        # ç»„åˆæ‰€æœ‰åˆ†æç»“æœ
        analysis_result = {
            'date': get_date_string(),
            'paper_count': len(papers),
            'keywords': keywords,
            'topics': topics,
            'statistics': statistics,
            'wordcloud_path': wordcloud_path,
            'llm_analysis': llm_analysis,
            'generated_at': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        self._save_analysis(analysis_result)
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("âœ… è¶‹åŠ¿åˆ†æå®Œæˆ")
        self.logger.info("=" * 60)
        
        return analysis_result
    
    def _extract_keywords(self, papers: List[Dict[str, Any]], top_n: int = 50) -> List[Dict[str, Any]]:
        """æå–å…³é”®è¯ï¼ˆä½¿ç”¨ TF-IDFï¼‰
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            top_n: è¿”å›å‰ N ä¸ªå…³é”®è¯
            
        Returns:
            å…³é”®è¯åˆ—è¡¨ï¼ŒåŒ…å«è¯å’Œæƒé‡
        """
        # åˆå¹¶æ‰€æœ‰è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦
        texts = []
        for paper in papers:
            text = paper['title'] + ' ' + paper['abstract']
            texts.append(text)
        
        # ä½¿ç”¨ TF-IDF æå–å…³é”®è¯
        vectorizer = TfidfVectorizer(
            max_features=top_n,
            stop_words=list(self.stop_words),
            ngram_range=(1, 2),  # åŒ…æ‹¬å•è¯å’ŒäºŒå…ƒç»„
            min_df=2,  # è‡³å°‘å‡ºç°åœ¨2ç¯‡è®ºæ–‡ä¸­
            max_df=0.8  # æœ€å¤šå‡ºç°åœ¨80%çš„è®ºæ–‡ä¸­
        )
        
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        
        # è®¡ç®—æ¯ä¸ªè¯çš„å¹³å‡ TF-IDF åˆ†æ•°
        avg_scores = tfidf_matrix.mean(axis=0).A1
        keyword_scores = list(zip(feature_names, avg_scores))
        keyword_scores.sort(key=lambda x: x[1], reverse=True)
        
        keywords = [
            {'keyword': kw, 'score': float(score)}
            for kw, score in keyword_scores[:top_n]
        ]
        
        self.logger.info(f"âœ“ æå–äº† {len(keywords)} ä¸ªå…³é”®è¯")
        self.logger.info(f"  Top 10: {', '.join([k['keyword'] for k in keywords[:10]])}")
        
        return keywords
    
    def _extract_topics(self, papers: List[Dict[str, Any]], n_topics: int = 5) -> List[Dict[str, Any]]:
        """æå–ä¸»é¢˜ï¼ˆä½¿ç”¨ LDAï¼‰
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            n_topics: ä¸»é¢˜æ•°é‡
            
        Returns:
            ä¸»é¢˜åˆ—è¡¨
        """
        # åˆå¹¶æ‰€æœ‰è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦
        texts = []
        for paper in papers:
            text = paper['title'] + ' ' + paper['abstract']
            texts.append(text)
        
        # ä½¿ç”¨ CountVectorizer
        vectorizer = CountVectorizer(
            max_features=1000,
            stop_words=list(self.stop_words),
            min_df=2,
            max_df=0.8
        )
        
        doc_term_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        
        # ä½¿ç”¨ LDA æå–ä¸»é¢˜
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            max_iter=20
        )
        lda.fit(doc_term_matrix)
        
        # æå–æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
        topics = []
        for topic_idx, topic in enumerate(lda.components_):
            top_indices = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_indices]
            topic_weight = topic[top_indices]
            
            topics.append({
                'topic_id': topic_idx + 1,
                'keywords': top_words,
                'weights': [float(w) for w in topic_weight]
            })
        
        self.logger.info(f"âœ“ æå–äº† {len(topics)} ä¸ªä¸»é¢˜")
        for i, topic in enumerate(topics, 1):
            self.logger.info(f"  ä¸»é¢˜ {i}: {', '.join(topic['keywords'][:5])}")
        
        return topics
    
    def _generate_wordcloud(self, papers: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆè¯äº‘å›¾
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            
        Returns:
            è¯äº‘å›¾ç‰‡è·¯å¾„
        """
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        text = ' '.join([
            paper['title'] + ' ' + paper['abstract']
            for paper in papers
        ])
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        
        # ç”Ÿæˆè¯äº‘
        wordcloud = WordCloud(
            width=1600,
            height=800,
            background_color='white',
            stopwords=self.stop_words,
            max_words=100,
            relative_scaling=0.5,
            colormap='viridis',
            min_font_size=10
        ).generate(text)
        
        # ä¿å­˜å›¾ç‰‡
        output_dir = Path('data/analysis')
        output_dir.mkdir(parents=True, exist_ok=True)
        
        date_str = get_date_string()
        wordcloud_path = f"data/analysis/wordcloud_{date_str}.png"
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(16, 8))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'Research Trends Word Cloud - {date_str}', fontsize=20, pad=20)
        plt.tight_layout(pad=0)
        
        # ä¿å­˜
        plt.savefig(wordcloud_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"âœ“ è¯äº‘å·²ä¿å­˜: {wordcloud_path}")
        
        return wordcloud_path
    
    def _generate_statistics(self, papers: List[Dict[str, Any]], 
                           summaries: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            summaries: è®ºæ–‡æ€»ç»“åˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # ç±»åˆ«ç»Ÿè®¡
        category_counts = Counter()
        for paper in papers:
            for category in paper.get('categories', []):
                category_counts[category] += 1
        
        # ä½œè€…ç»Ÿè®¡
        author_counts = Counter()
        for paper in papers:
            for author in paper.get('authors', []):
                author_counts[author] += 1
        
        # é«˜é¢‘è¯ç»Ÿè®¡
        word_counts = Counter()
        for paper in papers:
            title_words = paper['title'].lower().split()
            abstract_words = paper['abstract'].lower().split()
            for word in title_words + abstract_words:
                word = re.sub(r'[^\w]', '', word)
                if len(word) > 3 and word not in self.stop_words:
                    word_counts[word] += 1
        
        # æ—¶é—´åˆ†å¸ƒ
        time_distribution = Counter()
        for paper in papers:
            published = paper.get('published', '')
            if published:
                date = published.split('T')[0]
                time_distribution[date] += 1
        
        statistics = {
            'total_papers': len(papers),
            'total_authors': len(author_counts),
            'total_categories': len(category_counts),
            'category_distribution': dict(category_counts.most_common(10)),
            'top_authors': dict(author_counts.most_common(10)),
            'top_words': dict(word_counts.most_common(30)),
            'time_distribution': dict(sorted(time_distribution.items())),
            'prolific_authors': {k: v for k, v in author_counts.items() if v >= 2}
        }
        
        return statistics
    
    def _generate_llm_analysis(self, papers: List[Dict[str, Any]], 
                              summaries: List[Dict[str, Any]] = None,
                              keywords: List[Dict[str, Any]] = None,
                              topics: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM ç”Ÿæˆæ·±åº¦åˆ†æ
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            summaries: è®ºæ–‡æ€»ç»“åˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            
        Returns:
            LLM åˆ†æç»“æœ
        """
        if not self.llm_client:
            self.logger.warning("æœªæä¾› LLM å®¢æˆ·ç«¯ï¼Œè·³è¿‡æ·±åº¦åˆ†æ")
            return {
                'hotspots': 'éœ€è¦ LLM å®¢æˆ·ç«¯',
                'trends': 'éœ€è¦ LLM å®¢æˆ·ç«¯',
                'future_directions': 'éœ€è¦ LLM å®¢æˆ·ç«¯',
                'research_ideas': 'éœ€è¦ LLM å®¢æˆ·ç«¯'
            }
        
        # å‡†å¤‡è®ºæ–‡æ‘˜è¦ä¿¡æ¯
        papers_summary = []
        for i, paper in enumerate(papers[:30], 1):  # é™åˆ¶åœ¨å‰30ç¯‡
            summary_text = ""
            if summaries and i-1 < len(summaries):
                summary = summaries[i-1].get('summary', {})
                if isinstance(summary, dict):
                    summary_text = f"\n  å…³é”®åˆ›æ–°: {summary.get('key_innovation', '')}\n  ä¸»è¦æ–¹æ³•: {summary.get('main_method', '')}"
            
            papers_summary.append(
                f"{i}. {paper['title']}\n"
                f"  ç±»åˆ«: {', '.join(paper['categories'][:3])}"
                f"{summary_text}"
            )
        
        # å‡†å¤‡å…³é”®è¯ä¿¡æ¯
        top_keywords = [kw['keyword'] for kw in (keywords[:30] if keywords else [])]
        
        # å‡†å¤‡ä¸»é¢˜ä¿¡æ¯
        topic_descriptions = []
        if topics:
            for topic in topics:
                topic_descriptions.append(
                    f"ä¸»é¢˜ {topic['topic_id']}: {', '.join(topic['keywords'][:8])}"
                )
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_analysis_prompt(
            papers_summary='\n'.join(papers_summary),
            keywords=', '.join(top_keywords),
            topics='\n'.join(topic_descriptions),
            paper_count=len(papers)
        )
        
        try:
            self.logger.info("æ­£åœ¨ä½¿ç”¨ LLM ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            response = self.llm_client.generate(prompt, max_tokens=3000)
            
            # è§£æ LLM å“åº”
            analysis = self._parse_llm_response(response)
            
            self.logger.info("âœ“ LLM åˆ†æå®Œæˆ")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"LLM åˆ†æå¤±è´¥: {str(e)}")
            return {
                'hotspots': f'ç”Ÿæˆå¤±è´¥: {str(e)}',
                'trends': f'ç”Ÿæˆå¤±è´¥: {str(e)}',
                'future_directions': f'ç”Ÿæˆå¤±è´¥: {str(e)}',
                'research_ideas': f'ç”Ÿæˆå¤±è´¥: {str(e)}'
            }
    
    def _build_analysis_prompt(self, papers_summary: str, keywords: str, 
                              topics: str, paper_count: int) -> str:
        """æ„å»º LLM åˆ†ææç¤ºè¯
        
        Args:
            papers_summary: è®ºæ–‡æ‘˜è¦
            keywords: å…³é”®è¯
            topics: ä¸»é¢˜
            paper_count: è®ºæ–‡æ€»æ•°
            
        Returns:
            æç¤ºè¯å­—ç¬¦ä¸²
        """
        prompt = f"""ä½œä¸ºä¸€ä½èµ„æ·±çš„ AI ç ”ç©¶ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹ {paper_count} ç¯‡æœ€æ–°çš„ arXiv è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æã€‚

## è®ºæ–‡åˆ—è¡¨ï¼ˆå‰30ç¯‡ï¼‰ï¼š
{papers_summary}

## é«˜é¢‘å…³é”®è¯ï¼š
{keywords}

## ä¸»è¦ç ”ç©¶ä¸»é¢˜ï¼š
{topics}

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¿›è¡Œå…¨é¢åˆ†æï¼ˆç”¨ä¸­æ–‡å›ç­”ï¼‰ï¼š

### 1. å½“å‰ç ”ç©¶çƒ­ç‚¹åˆ†æ (Current Research Hotspots)
åˆ†æå½“å‰è¿™ä¸ªç ”ç©¶é¢†åŸŸæœ€çƒ­é—¨çš„3-5ä¸ªç ”ç©¶æ–¹å‘ï¼Œå¹¶è¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›æ–¹å‘å—åˆ°å…³æ³¨ã€‚

### 2. æŠ€æœ¯è¶‹åŠ¿ä¸æ¼”è¿› (Technical Trends)
è¯†åˆ«æŠ€æœ¯å‘å±•çš„è¶‹åŠ¿ï¼ŒåŒ…æ‹¬ï¼š
- ä¸»æµçš„æŠ€æœ¯æ–¹æ³•å’Œæ¶æ„
- æ­£åœ¨å…´èµ·çš„æ–°æŠ€æœ¯
- ä»è®ºæ–‡ä¸­è§‚å¯Ÿåˆ°çš„æŠ€æœ¯æ¼”è¿›è·¯å¾„

### 3. æœªæ¥å‘å±•æ–¹å‘ (Future Directions)
åŸºäºå½“å‰ç ”ç©¶çŠ¶å†µï¼Œé¢„æµ‹æœªæ¥6-12ä¸ªæœˆå¯èƒ½çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬ï¼š
- ç°æœ‰çƒ­ç‚¹çš„å¯èƒ½å»¶ä¼¸
- å°šæœªå……åˆ†æ¢ç´¢ä½†æœ‰æ½œåŠ›çš„æ–¹å‘
- å¯èƒ½çš„æŠ€æœ¯çªç ´ç‚¹

### 4. åˆ›æ–°ç ”ç©¶æƒ³æ³• (Research Ideas & Innovation Points)
æå‡º5-8ä¸ªå…·æœ‰åˆ›æ–°æ€§å’Œå¯è¡Œæ€§çš„ç ”ç©¶æƒ³æ³•ï¼Œæ¯ä¸ªæƒ³æ³•åŒ…æ‹¬ï¼š
- æ ¸å¿ƒåˆ›æ–°ç‚¹
- ä¸ºä»€ä¹ˆè¿™ä¸ªæƒ³æ³•æœ‰ä»·å€¼
- å¯èƒ½çš„æŠ€æœ¯å®ç°è·¯å¾„
- æ½œåœ¨çš„åº”ç”¨åœºæ™¯

è¯·ç¡®ä¿åˆ†æï¼š
- åŸºäºå®é™…è®ºæ–‡å†…å®¹
- å…·æœ‰æ·±åº¦å’Œæ´å¯ŸåŠ›
- æä¾›å¯æ“ä½œçš„ç ”ç©¶æ–¹å‘
- çªå‡ºåˆ›æ–°æ€§å’Œå‰ç»æ€§

è¯·ç”¨ Markdown æ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å’Œåˆ—è¡¨ã€‚"""

        return prompt
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æ LLM å“åº”
        
        Args:
            response: LLM å“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„åˆ†æç»“æœ
        """
        # å°è¯•æŒ‰ç…§æ ‡é¢˜åˆ†å‰²
        sections = {
            'hotspots': '',
            'trends': '',
            'future_directions': '',
            'research_ideas': '',
            'full_analysis': response
        }
        
        # ç®€å•çš„åˆ†å‰²é€»è¾‘ï¼ˆå¯ä»¥æ ¹æ®å®é™…å“åº”æ ¼å¼è°ƒæ•´ï¼‰
        current_section = None
        current_content = []
        
        for line in response.split('\n'):
            line_lower = line.lower().strip()
            
            if 'ç ”ç©¶çƒ­ç‚¹' in line or 'hotspot' in line_lower:
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                current_section = 'hotspots'
                current_content = [line]
            elif 'è¶‹åŠ¿' in line or 'trend' in line_lower:
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                current_section = 'trends'
                current_content = [line]
            elif 'æœªæ¥' in line or 'future' in line_lower or 'å‘å±•æ–¹å‘' in line:
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                current_section = 'future_directions'
                current_content = [line]
            elif 'åˆ›æ–°' in line or 'idea' in line_lower or 'æƒ³æ³•' in line:
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                current_section = 'research_ideas'
                current_content = [line]
            elif current_section:
                current_content.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªéƒ¨åˆ†
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        return sections
    
    def _save_analysis(self, analysis: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ
        
        Args:
            analysis: åˆ†æç»“æœ
        """
        output_dir = Path('data/analysis')
        output_dir.mkdir(parents=True, exist_ok=True)
        
        date_str = get_date_string()
        
        # ä¿å­˜ JSON æ ¼å¼
        json_path = f"data/analysis/analysis_{date_str}.json"
        save_json(analysis, json_path)
        self.logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜: {json_path}")
        
        # ä¿å­˜æœ€æ–°åˆ†æï¼ˆä¾› Web æœåŠ¡ä½¿ç”¨ï¼‰
        latest_path = "data/analysis/latest.json"
        save_json(analysis, latest_path)
        self.logger.info(f"ğŸ’¾ æœ€æ–°åˆ†æå·²ä¿å­˜: {latest_path}")
        
        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        markdown_path = f"data/analysis/report_{date_str}.md"
        self._generate_markdown_report(analysis, markdown_path)
        self.logger.info(f"ğŸ“„ Markdown æŠ¥å‘Šå·²ä¿å­˜: {markdown_path}")
    
    def _generate_markdown_report(self, analysis: Dict[str, Any], output_path: str):
        """ç”Ÿæˆ Markdown æ ¼å¼çš„åˆ†ææŠ¥å‘Š
        
        Args:
            analysis: åˆ†æç»“æœ
            output_path: è¾“å‡ºè·¯å¾„
        """
        llm_analysis = analysis.get('llm_analysis', {})
        statistics = analysis.get('statistics', {})
        keywords = analysis.get('keywords', [])
        
        report = f"""# ç ”ç©¶è¶‹åŠ¿åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¥æœŸ**: {analysis.get('date')}  
**åˆ†æè®ºæ–‡æ•°**: {analysis.get('paper_count')}  
**ç”Ÿæˆæ—¶é—´**: {analysis.get('generated_at')}

---

## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ

- **æ€»è®ºæ–‡æ•°**: {statistics.get('total_papers', 0)}
- **æ¶‰åŠä½œè€…æ•°**: {statistics.get('total_authors', 0)}
- **ç ”ç©¶ç±»åˆ«æ•°**: {statistics.get('total_categories', 0)}

### ç±»åˆ«åˆ†å¸ƒ

| ç±»åˆ« | è®ºæ–‡æ•° |
|------|--------|
"""
        
        for cat, count in list(statistics.get('category_distribution', {}).items())[:10]:
            report += f"| {cat} | {count} |\n"
        
        report += "\n### é«˜é¢‘å…³é”®è¯\n\n"
        for kw in keywords[:20]:
            report += f"- **{kw['keyword']}** (æƒé‡: {kw['score']:.4f})\n"
        
        report += "\n---\n\n## ğŸ”¥ ç ”ç©¶çƒ­ç‚¹åˆ†æ\n\n"
        report += llm_analysis.get('hotspots', 'æœªç”Ÿæˆ')
        
        report += "\n\n---\n\n## ğŸ“ˆ æŠ€æœ¯è¶‹åŠ¿ä¸æ¼”è¿›\n\n"
        report += llm_analysis.get('trends', 'æœªç”Ÿæˆ')
        
        report += "\n\n---\n\n## ğŸ”® æœªæ¥å‘å±•æ–¹å‘\n\n"
        report += llm_analysis.get('future_directions', 'æœªç”Ÿæˆ')
        
        report += "\n\n---\n\n## ğŸ’¡ åˆ›æ–°ç ”ç©¶æƒ³æ³•\n\n"
        report += llm_analysis.get('research_ideas', 'æœªç”Ÿæˆ')
        
        report += "\n\n---\n\n## ğŸ“¸ è¯äº‘å›¾\n\n"
        report += f"![è¯äº‘å›¾]({analysis.get('wordcloud_path', '')})\n"
        
        report += "\n---\n\n*æœ¬æŠ¥å‘Šç”± Daily arXiv Agent è‡ªåŠ¨ç”Ÿæˆ*\n"
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
    
    def print_analysis_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°åˆ†ææ‘˜è¦
        
        Args:
            analysis: åˆ†æç»“æœ
        """
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ“Š è¶‹åŠ¿åˆ†ææ‘˜è¦")
        self.logger.info("=" * 80)
        
        statistics = analysis.get('statistics', {})
        keywords = analysis.get('keywords', [])
        
        self.logger.info(f"\næ€»è®ºæ–‡æ•°: {statistics.get('total_papers', 0)}")
        self.logger.info(f"æ¶‰åŠä½œè€…: {statistics.get('total_authors', 0)}")
        self.logger.info(f"ç ”ç©¶ç±»åˆ«: {statistics.get('total_categories', 0)}")
        
        self.logger.info("\né«˜é¢‘å…³é”®è¯ (Top 15):")
        for kw in keywords[:15]:
            self.logger.info(f"  - {kw['keyword']}")
        
        self.logger.info("\nç±»åˆ«åˆ†å¸ƒ:")
        for cat, count in list(statistics.get('category_distribution', {}).items())[:8]:
            self.logger.info(f"  - {cat}: {count} ç¯‡")
        
        llm_analysis = analysis.get('llm_analysis', {})
        if llm_analysis.get('full_analysis'):
            self.logger.info("\n" + "=" * 80)
            self.logger.info("ğŸ¤– LLM æ·±åº¦åˆ†æ")
            self.logger.info("=" * 80)
            # åªæ‰“å°å‰500ä¸ªå­—ç¬¦
            full_text = llm_analysis.get('full_analysis', '')
            preview = full_text[:500] + "..." if len(full_text) > 500 else full_text
            self.logger.info(f"\n{preview}")
            self.logger.info(f"\nå®Œæ•´åˆ†æè¯·æŸ¥çœ‹: data/analysis/report_{analysis.get('date')}.md")
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info(f"ğŸ“¸ è¯äº‘å›¾: {analysis.get('wordcloud_path')}")
        self.logger.info("=" * 80 + "\n")


def main():
    """æµ‹è¯•å‡½æ•°"""
    from src.utils import load_config, load_env, setup_logging, load_json
    from src.summarizer.llm_factory import LLMClientFactory
    
    load_env()
    config = load_config()
    logger = setup_logging(config)
    
    # åŠ è½½è®ºæ–‡æ•°æ®
    papers_data = load_json('data/papers/latest.json')
    if not papers_data:
        logger.error("æœªæ‰¾åˆ°è®ºæ–‡æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œè®ºæ–‡çˆ¬å–")
        return
    
    papers = papers_data.get('papers', [])
    
    # åŠ è½½æ€»ç»“æ•°æ®
    summaries_data = load_json('data/summaries/latest.json')
    summaries = summaries_data.get('summaries', []) if summaries_data else None
    
    # åˆ›å»º LLM å®¢æˆ·ç«¯
    llm_client = LLMClientFactory.create_client(config)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = TrendAnalyzer(config, llm_client)
    
    # æ‰§è¡Œåˆ†æ
    analysis = analyzer.analyze(papers, summaries)
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_analysis_summary(analysis)


if __name__ == "__main__":
    main()
