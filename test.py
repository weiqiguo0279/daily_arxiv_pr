"""
Daily arXiv Agent - ä¸»ç¨‹åºå…¥å£

æ¯æ—¥è¿½è¸ª arXiv æœ€æ–°è®ºæ–‡ï¼Œä½¿ç”¨ LLM è¿›è¡Œæ€»ç»“å’Œåˆ†æ
"""
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils import load_config, load_env, setup_logging, get_date_string, get_github_token
from src.pr import create_github_pr, merge_github_pr
from src.upload import upload_to_github
from src.notifier.email_notifier import EmailNotifier


def main():
    """ä¸»å‡½æ•°"""
    # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åŠ è½½é…ç½®
    load_env()
    config = load_config()
    gh_token = get_github_token()
    logger = setup_logging(config)
    
    # åˆå§‹åŒ–é‚®ä»¶é€šçŸ¥å™¨
    notifier = None
    if config.get('scheduler', {}).get('notification', {}).get('enabled', False):
        notifier = EmailNotifier(config['scheduler']['notification']['email'])
    
    logger.info("=" * 60)
    logger.info("Daily arXiv Agent å¯åŠ¨")
    logger.info(f"æ—¥æœŸ: {get_date_string()}")
    logger.info("=" * 60)
    
    # ä»»åŠ¡çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
    task_success = False
    stats = {}
    error_msg = None
    
    try:
        # ç¬¬ä¸€æ­¥ - å®ç°è®ºæ–‡çˆ¬å– âœ…
        logger.info("æ­¥éª¤ 1: çˆ¬å– arXiv è®ºæ–‡...")
        from src.crawler.arxiv_fetcher import ArxivFetcher
        fetcher = ArxivFetcher(config)
        
        # å°è¯•è·å–è®ºæ–‡ï¼Œå¦‚æœæ²¡æ‰¾åˆ°ï¼Œé€æ­¥æ”¾å®½æ¡ä»¶
        # papers = fetcher.fetch_papers(days_back=210)
        papers = fetcher.fetch_papers(days_back=3)
        if not papers:
            logger.warning("âš ï¸  è¿‡å»2å¤©æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡...")
            # logger.warning("âš ï¸  è¿‡å»2å¤©æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œå°è¯•æ‰©å¤§åˆ°7å¤©...")
            # papers = fetcher.fetch_papers(days_back=7)
        
        if papers:
            fetcher.print_paper_summary(papers)
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            stats['papers_count'] = len(papers)
            stats['categories_count'] = len(config.get('arxiv', {}).get('categories', []))
            stats['keywords_count'] = len(config.get('arxiv', {}).get('keywords', []))
        else:
            logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
            logger.info("ğŸ’¡ æç¤º: å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š")
            logger.info("   1. åœ¨ config.yaml ä¸­å¢åŠ  days_back æˆ– max_results")
            logger.info("   2. å‡å°‘æˆ–åˆ é™¤å…³é”®è¯è¿‡æ»¤ï¼ˆè®¾ç½® keywords: []ï¼‰")
            logger.info("   3. ä¿®æ”¹ç±»åˆ«èŒƒå›´")
            return
        
        # ç¬¬äºŒæ­¥ - å®ç°è®ºæ–‡æ€»ç»“ âœ…
        logger.info("\næ­¥éª¤ 2: æ€»ç»“è®ºæ–‡...")
        from src.summarizer.paper_summarizer import PaperSummarizer
        
        summarized_papers = None
        try:
            summarizer = PaperSummarizer(config)
            summarized_papers = summarizer.summarize_papers(papers)
            
            # ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
            logger.info("\nç”Ÿæˆæ¯æ—¥æŠ¥å‘Š...")
            report = summarizer.generate_daily_report(summarized_papers)
            
            # ä¿å­˜æŠ¥å‘Š
            report_path = f"data/summaries/report_{get_date_string()}.md"
            from pathlib import Path
            Path(report_path).parent.mkdir(parents=True, exist_ok=True)
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ğŸ“„ æ¯æ—¥æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            stats['summaries_count'] = len(summarized_papers)
            
        except Exception as e:
            logger.error(f"è®ºæ–‡æ€»ç»“å¤±è´¥: {str(e)}")
            logger.info("ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤...")
            summarized_papers = papers


        # ç¬¬ä¸‰æ­¥â€”â€”åˆ›å»º GitHub PR âœ…
        logger.info("\næ­¥éª¤ 3: åˆ›å»º GitHub PR...")
        try:
            if gh_token and config.get('github'):
                logger.info("\nåˆ›å»º GitHub PR...")
                report_path = f"data/summaries/report_{get_date_string()}.md"
                #create_github_pr(config, gh_token, report_path)
                #merge_github_pr(config, gh_token, report_path)
                upload_to_github(config, gh_token, report_path)
        except Exception as e:
            logger.error(f"åˆ›å»º PR å¤±è´¥: {str(e)}")
        
        # ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ
        task_success = True
        
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå‡ºé”™: {str(e)}", exc_info=True)
        error_msg = str(e)
        sys.exit(1)
    finally:
        # è®¡ç®—ä»»åŠ¡æ‰§è¡Œæ—¶é—´
        duration = time.time() - start_time
        
        # å‘é€é‚®ä»¶é€šçŸ¥
        if notifier:
            logger.info("\næ­¥éª¤ 4: å‘é€é‚®ä»¶é€šçŸ¥...")
            try:
                notifier.send_notification(
                    success=task_success,
                    stats=stats,
                    error_msg=error_msg,
                    duration=duration
                )
            except Exception as e:
                logger.error(f"é‚®ä»¶é€šçŸ¥å‘é€å¤±è´¥: {str(e)}")
        
        # è®°å½•ä»»åŠ¡å®Œæˆä¿¡æ¯
        if task_success:
            logger.info("=" * 60)
            logger.info("âœ… Daily arXiv Agent ä»»åŠ¡å®Œæˆ")
            logger.info(f"æ‰§è¡Œæ—¶é—´: {duration:.2f} ç§’")
            logger.info("=" * 60)
        else:
            logger.info("=" * 60)
            logger.info("âŒ Daily arXiv Agent ä»»åŠ¡å¤±è´¥")
            logger.info(f"æ‰§è¡Œæ—¶é—´: {duration:.2f} ç§’")
            logger.info("=" * 60)


# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
if __name__ == "__main__":
    main()